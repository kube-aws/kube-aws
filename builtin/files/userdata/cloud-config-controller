{{ define "instance-script" -}}
{{- $S3URI := self.Parts.s3.Asset.S3URL -}}
 . /etc/environment
export COREOS_PRIVATE_IPV4 COREOS_PRIVATE_IPV6 COREOS_PUBLIC_IPV4 COREOS_PUBLIC_IPV6
REGION=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
USERDATA_FILE=userdata-controller

run() {
  bin="$1"; shift
  while ! /usr/bin/rkt run \
          --net=host \
          --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf  \
          --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false --mount volume=awsenv,target=/var/run/coreos \
          --trust-keys-from-https \
          {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=$bin -- "$@"; do
    sleep 1
  done
}

run bash -c "aws configure set s3.signature_version s3v4; aws s3 --region $REGION cp {{$S3URI}} /var/run/coreos/$USERDATA_FILE"

INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
run bash -c "aws ec2 modify-instance-attribute --no-source-dest-check --instance-id $INSTANCE_ID --region $REGION"

{{ .NodeProvisioner.RemoteCommand }}

exec /usr/bin/coreos-cloudinit --from-file /var/run/coreos/$USERDATA_FILE
{{ end }}

{{ define "instance" -}}
{ "Fn::Base64": { "Fn::Join" : ["\n", [
  "#!/bin/bash -xe",
  "# s3-part-fingerprint: {{ (execTemplate "s3" .) | fingerprint }}",
  {"Fn::Sub": "echo '{{.StackNameEnvVarName}}=${AWS::StackName}' >>{{.StackNameEnvFileName}}"},
  {{ (execTemplate "instance-script" .) | toJSON }}
]]}}
{{ end }}

{{ define "s3" -}}
#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  units:
{{if .DisableContainerLinuxAutomaticUpdates}}
    - name: update-engine.service
      mask: true
    - name: locksmithd.service
      mask: true
{{end}}

    - name: handle-disable-request.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Shuts down core services when requested
        After=kubelet.service network-online.target
        Wants=kubelet.service

        [Service]
        Type=simple
        TimeoutStartSec=60m
        Restart=on-failure
        RestartSec=30
        ExecStartPre=/usr/bin/systemctl is-active kubelet
        ExecStart=/opt/bin/handle-disable-request

        [Install]
        WantedBy=multi-user.target

    - name: handle-cluster-cidr-changes.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Handle changes to pod and service cidrs
        After=kubelet.service network-online.target
        Wants=kubelet.service

        [Service]
        Type=simple
        TimeoutStartSec=60m
        Restart=on-failure
        RestartSec=30
        ExecStartPre=/usr/bin/systemctl is-active kubelet
        ExecStart=/opt/bin/handle-cluster-cidr-changes

        [Install]
        WantedBy=multi-user.target

{{- range $u := .Controller.CustomSystemdUnits}}
    - name: {{$u.Name}}
      {{- if $u.Command }}
      command: {{ $u.Command }}
      {{- end}}
      {{- if $u.Enable }}
      enable: {{ $u.Enable }}
      {{- end }}
      {{- if $u.Runtime }}
      runtime: {{ $u.Runtime }}
      {{- end }}
      {{- if $u.DropInsPresent }}
      drop-ins:
        {{- range $d := $u.DropIns }}
        - name: {{ $d.Name }}
          content: |
            {{- range $i := $d.ContentArray }}
            {{ $i }}
            {{- end}}
        {{- end }}
      {{- end}}
      {{- if $u.ContentPresent }}
      content: |
        {{- range $l := $u.ContentArray}}
        {{ $l }}
        {{- end }}
      {{- end }}
{{- end}}
    - name: systemd-modules-load.service
      command: restart
{{range $volumeMountSpecIndex, $volumeMountSpec := .Controller.VolumeMounts}}
    - name: format-{{$volumeMountSpec.SystemdMountName}}.service
      command: start
      content: |
        [Unit]
        Description=Formats the EBS persistent volume drive for {{$volumeMountSpec.Device}}
        Before=local-fs-pre.target

        [Service]
        Type=oneshot
        ExecStart=-/usr/sbin/mkfs.xfs {{$volumeMountSpec.Device}}

        [Install]
        WantedBy=local-fs-pre.target

    - name: {{$volumeMountSpec.SystemdMountName}}.mount
      command: start
      content: |
        [Unit]
        Description=Mount volume to {{$volumeMountSpec.Path}}

        [Mount]
        What={{$volumeMountSpec.Device}}
        Where={{$volumeMountSpec.Path}}
{{end}}
{{- if .HostOS.BashPrompt.Enabled }}
    - name: replace-prompt.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Replaces the default bashrc with more informative prompt
        Before=sshd.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=/bin/bash -c "mv /etc/bash/bashrc /etc/old-bashrc"
        ExecStart=/bin/bash -c "cp /etc/bash/bashrc-kube-aws /etc/bash/bashrc"

        [Install]
        WantedBy=multi-user.target
{{- end }}
{{- if .HostOS.MOTDBanner.Enabled }}
    - name: enhance-motd.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Add additional information to system motd
        Before=sshd.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/bin/bash -c "echo -e \"{{.HostOS.MOTDBanner.KubernetesColour.On}}Kubernetes{{.HostOS.MOTDBanner.KubernetesColour.Off}} {{ .K8sVer }}\" >>/etc/motd"


        [Install]
        WantedBy=multi-user.target
{{- end }}
{{if and (.AmazonSsmAgent.Enabled) (ne .AmazonSsmAgent.DownloadUrl "")}}
    - name: amazon-ssm-agent.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=amazon-ssm-agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        Type=simple
        ExecStartPre=/opt/ssm/bin/install-ssm-agent.sh
        ExecStart=/opt/ssm/bin/amazon-ssm-agent
        KillMode=controll-group
        Restart=on-failure
        RestartSec=1min

        [Install]
        WantedBy=network-online.target
{{end}}
{{if .CloudWatchLogging.Enabled}}
    - name: journald-cloudwatch-logs.service
      command: start
      content: |
        [Unit]
        Description=Docker run journald-cloudwatch-logs to send journald logs to CloudWatch
        Requires=network-online.target
        After=network-online.target

        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /var/journald-cloudwatch-logs
        ExecStart=/usr/bin/rkt run \
                  --insecure-options=image \
                  --volume resolv,kind=host,source=/etc/resolv.conf,readOnly=true \
                  --mount volume=resolv,target=/etc/resolv.conf \
                  --volume journald-cloudwatch-logs,kind=host,source=/var/journald-cloudwatch-logs \
                  --mount volume=journald-cloudwatch-logs,target=/var/journald-cloudwatch-logs \
                  --volume journal,kind=host,source=/var/log/journal,readOnly=true \
                  --mount volume=journal,target=/var/log/journal \
                  --volume machine-id,kind=host,source=/etc/machine-id,readOnly=true \
                  --mount volume=machine-id,target=/etc/machine-id \
                  --uuid-file-save=/var/journald-cloudwatch-logs/journald-cloudwatch-logs.uuid \
                  {{ .JournaldCloudWatchLogsImage.RktRepo }} -- {{.ClusterName}}
        ExecStopPost=/usr/bin/rkt rm --uuid-file=/var/journald-cloudwatch-logs/journald-cloudwatch-logs.uuid
        Restart=always
        RestartSec=60s

        [Install]
        WantedBy=multi-user.target
{{end}}
    - name: cfn-etcd-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Fetches etcd static IP addresses list from CF
        After=network-online.target

        [Service]
        Restart=on-failure
        RemainAfterExit=true
        EnvironmentFile={{.StackNameEnvFileName}}
        ExecStartPre=/opt/bin/cfn-etcd-environment
        ExecStart=/usr/bin/mv -f /var/run/coreos/etcd-environment /etc/etcd-environment
{{if .Experimental.AwsEnvironment.Enabled}}
    - name: set-aws-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Set AWS environment variables in /etc/aws-environment
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        EnvironmentFile={{.StackNameEnvFileName}}
        ExecStartPre=/bin/touch /etc/aws-environment
        ExecStart=/opt/bin/set-aws-environment
{{end}}

{{ if .CfnInitConfigSets }}
    - name: cfn-init-files.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Render necessary files with cfn-init
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        EnvironmentFile={{.StackNameEnvFileName}}
        ExecStart=/opt/bin/cfn-init-files
{{ end }}

    - name: docker.service
      drop-ins:
{{if .Experimental.EphemeralImageStorage.Enabled}}
        - name: 10-docker-mount.conf
          content: |
            [Unit]
            After=var-lib-docker.mount
            Wants=var-lib-docker.mount
{{end}}
        - name: 10-post-start-check.conf
          content: |
            [Service]
            RestartSec=10
            ExecStartPost=/usr/bin/docker pull {{.PauseImage.RepoWithTag}}

        - name: 60-logfilelimit.conf
          content: |
            [Service]
            Environment="DOCKER_OPTS=--log-opt max-size=50m --log-opt max-file=3"

    - name: flanneld.service
      enable: false
    {{ if .AssetsEncryptionEnabled -}}
    - name: decrypt-assets.service
      enable: true
      command: start
      content: |
        [Unit]
        Description=Convert encrypted credentials

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/opt/bin/retry 10 /opt/bin/decrypt-assets
    {{- end }}

    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=cfn-etcd-environment.service
        After=cfn-etcd-environment.service
        Wants=rpc-statd.service
        Wants=decrypt-assets.service
        After=decrypt-assets.service

        [Service]

        # EnvironmentFile=/etc/environment allows the reading of COREOS_PRIVATE_IPV4
        EnvironmentFile=/etc/environment
        EnvironmentFile=-/etc/etcd-environment
        EnvironmentFile=-/etc/default/kubelet
        ExecStartPre=/usr/bin/systemctl is-active cfn-etcd-environment.service
        ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/cni/net.d
        ExecStartPre=/usr/bin/mkdir -p /var/run/calico
        ExecStartPre=/usr/bin/mkdir -p /var/lib/calico
        ExecStartPre=/usr/bin/mkdir -p /var/lib/kubelet
        ExecStartPre=/usr/bin/mkdir -p /var/lib/docker
        ExecStartPre=/bin/bash -c "if ! grep -q "/var/lib/kubelet" /proc/mounts; then mount --bind /var/lib/kubelet /var/lib/kubelet && mount --make-shared /var/lib/kubelet; fi"
        ExecStartPre=/bin/sed -e "s/COREOS_PRIVATE_IPV4/${COREOS_PRIVATE_IPV4}/g" -i /etc/kubernetes/config/kubelet.yaml
        ExecStartPre=/bin/sh -ec "find /etc/kubernetes/manifests /srv/kubernetes/manifests  -maxdepth 1 -type f | xargs --no-run-if-empty sed -i 's|#ETCD_ENDPOINTS#|${ETCD_ENDPOINTS}|'"
        ExecStartPre=-/bin/docker rm -f kubelet
        ExecStart=/bin/sh -c "docker run --name kubelet --privileged --net=host --pid=host \
        -v /:/rootfs:ro \
        -v /sys:/sys:ro \
        -v /dev:/dev \
        -v /var/run/docker.sock:/var/run/docker.sock \
        -v /etc/resolv.conf:/etc/resolv.conf:ro \
        -v /var/lib/cni:/var/lib/cni:rw \
        -v /var/run/calico:/var/run/calico:rw \
        -v /var/lib/calico:/var/lib/calico:rw \
        -v /var/log:/var/log:rw \
        -v /opt/cni/bin:/opt/cni/bin:rw \
        -v /etc/kubernetes:/etc/kubernetes:rw \
        -v /var/lib/kubelet:/var/lib/kubelet:shared \
        -v /var/lib/docker:/var/lib/docker:rshared \
        {{- if gt (len .Kubelet.Mounts) 0 }}
          {{- range .Kubelet.Mounts }}
        {{ .MountDockerRW }} \
          {{- end }}
        {{- end }}
        {{ .HyperkubeImage.RepoWithTag }} /kubelet \
        {{- if checkVersion ">= 1.14" .K8sVer }}
        --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig/worker-bootstrap.yaml \
        {{- end }}
        {{ if .Kubelet.Kubeconfig -}}
        --kubeconfig={{ .Kubelet.Kubeconfig }} \
        {{ else -}}
        --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.yaml \
        {{ end -}}
        {{if checkVersion "<1.10" .K8sVer -}}
        --require-kubeconfig \
        {{end -}}
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --cni-bin-dir=/opt/cni/bin \
        --network-plugin={{.K8sNetworkPlugin}} \
        --container-runtime={{.ContainerRuntime}} \
        --register-node=true \
        --node-labels=node.kubernetes.io/role="master",service-cidr={{ .ServiceCIDR | toLabel }}{{if .NodeLabels.Enabled}},{{.NodeLabels.String}}{{end}} \
        --register-with-taints=node.kubernetes.io/role=master:NoSchedule \
        --config=/etc/kubernetes/config/kubelet.yaml \
        {{- if .Experimental.CloudControllerManager.Enabled }}
        --cloud-provider=external \
        {{- else }}
        --cloud-provider=aws \
        {{- end }}
        {{- if .Kubernetes.Networking.AmazonVPC.Enabled }}
        --node-ip=$$(curl http://169.254.169.254/latest/meta-data/local-ipv4) \
        --max-pods=$$(/opt/bin/aws-k8s-cni-max-pods) \
        {{- end }}
        {{- range $f := .Kubelet.Flags}}
        --{{$f.Name}}={{$f.Value}} \
        {{- end }}
        $KUBELET_OPTS"
        ExecStop=/usr/bin/docker rm -f kubelet
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

    - name: rpc-statd.service
      command: start
      enable: true

    - name: install-kube-system.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=kubelet.service docker.service

        [Service]
        Type=oneshot
        StartLimitInterval=0
        RemainAfterExit=true
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/systemctl is-active kubelet.service; do echo waiting until kubelet starts; sleep 10; done"
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/systemctl is-active docker.service; do echo waiting until docker starts; sleep 10; done"

        # FIXME: Remove dependency on the apiserver insecure port
        ExecStartPre=/usr/bin/bash -c "until /usr/bin/curl -s -k https://127.0.0.1/version; do echo waiting until apiserver starts; sleep 10; done"

        ExecStart=/opt/bin/retry 10 /opt/bin/install-kube-system

{{ if $.ElasticFileSystemID }}
    - name: efs.service
      command: start
      content: |
        [Unit]
        After=network-online.target
        Before=kubelet.service
        [Service]
        Type=oneshot
        ExecStartPre=-/usr/bin/mkdir -p /efs
        ExecStart=/bin/sh -c 'grep -qs /efs /proc/mounts || /usr/bin/mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 $(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone).{{ $.ElasticFileSystemID }}.efs.{{ $.Region }}.amazonaws.com:/ /efs'
        ExecStop=/usr/bin/umount /efs
        RemainAfterExit=yes
        [Install]
        WantedBy=kubelet.service
{{ end }}
{{if .WaitSignal.Enabled}}
    - name: cfn-signal.service
      command: start
      content: |
        [Unit]
        Wants=kubelet.service docker.service install-kube-system.service
        After=kubelet.service install-kube-system.service

        [Service]
        Type=simple
        Restart=on-failure
        RestartSec=60
        StartLimitInterval=640
        StartLimitBurst=10
        EnvironmentFile={{.StackNameEnvFileName}}
        ExecStartPre=/usr/bin/systemctl is-active install-kube-system.service
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl -s -m 20 -k  https://127.0.0.1/healthz > /dev/null &&  /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10252/healthz > /dev/null && /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10251/healthz > /dev/null &&  /usr/bin/curl --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null && /usr/bin/curl -s -m 20 -f http://127.0.0.1:10256/healthz > /dev/null; then break ; fi;  done"

        {{if .Experimental.AuditLog.Enabled -}}
        ExecStartPre=/opt/bin/check-worker-communication
        {{end -}}
        ExecStart=/opt/bin/cfn-signal
{{end}}
{{if .Experimental.AwsNodeLabels.Enabled }}
    - name: kube-node-label.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Label this kubernetes node with additional AWS parameters
        Wants=kubelet.service
        After=kubelet.service
        Before=cfn-signal.service

        [Service]
        Type=oneshot
        ExecStop=/bin/true
        RemainAfterExit=true
        ExecStart=/opt/bin/kube-node-label
{{end}}

{{if .Experimental.EphemeralImageStorage.Enabled}}
    - name: format-ephemeral.service
      command: start
      content: |
        [Unit]
        Description=Formats the ephemeral drive
        ConditionFirstBoot=yes
        After=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        Requires=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/sbin/wipefs -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
        ExecStart=/usr/sbin/mkfs.{{.Experimental.EphemeralImageStorage.Filesystem}} -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
    - name: var-lib-docker.mount
      command: start
      content: |
        [Unit]
        Description=Mount ephemeral to /var/lib/docker
        Requires=format-ephemeral.service
        After=format-ephemeral.service
        [Mount]
        What=/dev/{{.Experimental.EphemeralImageStorage.Disk}}
{{if eq .ContainerRuntime "docker"}}
        Where=/var/lib/docker
{{end}}
        Type={{.Experimental.EphemeralImageStorage.Filesystem}}
{{end}}
{{ if .SharedPersistentVolume }}
    - name: load-efs-pv.service
      command: start
      content: |
        [Unit]
        Description=Load efs persistent volume mount
        Wants=kubelet.service
        After=kubelet.service
        Before=cfn-signal.service
        [Service]
        Type=simple
        EnvironmentFile={{.StackNameEnvFileName}}
        RemainAfterExit=true
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/opt/bin/set-efs-pv
        ExecStart=/opt/bin/load-efs-pv
{{end}}

{{if .SSHAuthorizedKeys}}
ssh_authorized_keys:
  {{range $sshkey := .SSHAuthorizedKeys}}
  - {{$sshkey}}
  {{end}}
{{end}}

{{if .Region.IsChina}}
    - name: pause-amd64.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Pull and tag a mirror image for pause-amd64
        Wants=docker.service
        After=docker.service

        [Service]
        Restart=on-failure
        RemainAfterExit=true
        ExecStartPre=/usr/bin/systemctl is-active docker.service
        ExecStartPre=/usr/bin/docker pull {{.PauseImage.RepoWithTag}}
        ExecStart=/usr/bin/docker tag {{.PauseImage.RepoWithTag}} gcr.io/google_containers/pause-amd64:3.0
        ExecStop=/bin/true
        [Install]
        WantedBy=install-kube-system.service
{{end}}
write_files:
  - path: /etc/ssh/sshd_config
    permissions: 0600
    owner: root:root
    content: |
      UsePrivilegeSeparation sandbox
      Subsystem sftp internal-sftp
      ClientAliveInterval 180
      UseDNS no
      UsePAM yes
      PrintLastLog no # handled by PAM
      PrintMotd no # handled by PAM
      PasswordAuthentication no
      ChallengeResponseAuthentication no

  {{ if .Controller.CustomFiles -}}
  {{ range $i, $w := .Controller.CustomFiles -}}
  - path: {{$w.Path}}{{ if $w.Encrypted }}.enc{{end}}
    permissions: {{$w.PermissionsString}}
    encoding: gzip+base64
    content: {{$w.RenderGzippedBase64Content $}}
  {{ end -}}
  {{ end -}}
  - path: /etc/modules-load.d/ip_vs.conf
    content: |
      ip_vs
      ip_vs_rr
      ip_vs_wrr
      ip_vs_sh
      nf_conntrack_ipv4

  - path: /etc/kubernetes/config/kubelet.yaml
    content: |
      kind: KubeletConfiguration
      apiVersion: kubelet.config.k8s.io/v1beta1
      evictionHard:
        memory.available:  "200Mi"
      staticPodPath: /etc/kubernetes/manifests
      clusterDomain: cluster.local
      clusterDNS:
      - {{ if .KubeDns.NodeLocalResolver }}COREOS_PRIVATE_IPV4{{ else }}{{.DNSServiceIP}}{{end}}
      serverTLSBootstrap: false
      rotateCertificates: true
      authentication:
        anonymous:
          enabled: true
        webhook:
          enabled: true
          cacheTTL: "2m"
      authorization:
        mode: Webhook
        webhook:
          cacheAuthorizedTTL: "2m"
          cacheUnauthorizedTTL: "1m"
      {{- if .Kubelet.SystemReservedResources }}
      systemReserved: {{ .Kubelet.SystemReservedResources }}
      {{- end }}
      {{- if .Kubelet.KubeReservedResources }}
      kubeReserved: {{ .Kubelet.KubeReservedResources }}
      {{- end }}
      {{- if .ControllerFeatureGates.Enabled }}
      featureGates:
{{.ControllerFeatureGates.Yaml | indent 8}}
      {{- end }}

{{if and (.AmazonSsmAgent.Enabled) (ne .AmazonSsmAgent.DownloadUrl "")}}
  - path: "/opt/ssm/bin/install-ssm-agent.sh"
    permissions: 0700
    content: |
      #!/bin/bash
      set -e

      TARGET_DIR=/opt/ssm
      if [[ -f "${TARGET_DIR}"/bin/amazon-ssm-agent ]]; then
        exit 0
      fi

      TMP_DIR=$(mktemp -d)
      trap "rm -rf ${TMP_DIR}" EXIT

      TAR_FILE=ssm.linux-amd64.tar.gz
      CHECKSUM_FILE="${TAR_FILE}.sha1"

      echo -n "{{ .AmazonSsmAgent.Sha1Sum }} ${TMP_DIR}/${TAR_FILE}" > "${TMP_DIR}/${CHECKSUM_FILE}"

      curl --silent -L -o "${TMP_DIR}/${TAR_FILE}" "{{ .AmazonSsmAgent.DownloadUrl }}"

      sha1sum --quiet -c "${TMP_DIR}/${CHECKSUM_FILE}"

      tar zfx "${TMP_DIR}"/"${TAR_FILE}" -C "${TMP_DIR}"
      chown -R root:root "${TMP_DIR}"/ssm

      CONFIG_DIR=/etc/amazon/ssm
      mkdir -p "${CONFIG_DIR}"
      mv -f "${TMP_DIR}"/ssm/amazon-ssm-agent.json "${CONFIG_DIR}"/amazon-ssm-agent.json
      mv -f "${TMP_DIR}"/ssm/seelog_unix.xml "${CONFIG_DIR}"/seelog.xml

      mv -f "${TMP_DIR}"/ssm/* "${TARGET_DIR}"/bin/

{{end}}
{{if .Experimental.DisableSecurityGroupIngress}}
  - path: /etc/kubernetes/cloud-controller-manager/cloud.config
    owner: root:root
    permissions: 0644
    content: |
      [global]
      DisableSecurityGroupIngress = true
{{end}}

{{if .Experimental.AwsEnvironment.Enabled}}
  - path: /opt/bin/set-aws-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/etc/aws-environment,readOnly=false \
        --mount volume=awsenv,target=/etc/aws-environment \
        --uuid-file-save=/var/run/coreos/set-aws-environment.uuid \
        --set-env={{.StackNameEnvVarName}}=${{.StackNameEnvVarName}} \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          '
           cfn-init -v -c "aws-environment" --region {{.Region}} --resource {{.Controller.LogicalName}} --stack "${{.StackNameEnvVarName}}"
          '

      rkt rm --uuid-file=/var/run/coreos/set-aws-environment.uuid || :
{{end}}

{{ if .CfnInitConfigSets }}
  - path: /opt/bin/cfn-init-files
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      id=$(basename $0)

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=srv-kube-aws-plugins,kind=host,source=/srv/kube-aws/plugins,readOnly=false \
        --mount volume=srv-kube-aws-plugins,target=/srv/kube-aws/plugins \
        --uuid-file-save=/var/run/coreos/${id}.uuid \
        --set-env={{.StackNameEnvVarName}}=${{.StackNameEnvVarName}} \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          '
          {{ range $k, $_ := .CfnInitConfigSets -}}
           cfn-init -v -c "{{ $k }}" --region {{$.Region}} --resource {{$.Controller.LogicalName}} --stack "${{$.StackNameEnvVarName}}"
          {{ end -}}
          '

      rkt rm --uuid-file=/var/run/coreos/${id}.uuid || :
{{ end }}

  {{if .Experimental.AwsNodeLabels.Enabled -}}
  - path: /opt/bin/kube-node-label
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      set -ue

      INSTANCE_ID="$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/instance-id)"
      SECURITY_GROUPS="$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/security-groups | tr '\n' ',')"
      AUTOSCALINGGROUP="$(/usr/bin/docker run --rm --net=host \
                {{.AWSCliImage.RepoWithTag}} aws \
                autoscaling describe-auto-scaling-instances \
                --instance-ids ${INSTANCE_ID} --region {{.Region}} \
                --query 'AutoScalingInstances[].AutoScalingGroupName' --output text)"
      LAUNCHCONFIGURATION="$(/usr/bin/docker run --rm --net=host \
                {{.AWSCliImage.RepoWithTag}} \
                aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-name $AUTOSCALINGGROUP --region {{.Region}} \
                --query 'AutoScalingGroups[].LaunchConfigurationName' --output text)"

       # FIXME: Remove dependency on the apiserver insecure port
       until /usr/bin/curl -s -f http://127.0.0.1:8080/version; do echo waiting until apiserver starts; sleep 1; done

       # FIXME: Remove dependency on the apiserver insecure port
       /usr/bin/curl \
         --retry 5 \
         --request PATCH \
         -H 'Content-Type: application/strategic-merge-patch+json' \
         -d '{
              "metadata": {
                "labels": {
                  "kube-aws.coreos.com/autoscalinggroup": "'${AUTOSCALINGGROUP}'",
                   "kube-aws.coreos.com/launchconfiguration": "'${LAUNCHCONFIGURATION}'"
                 },
                 "annotations": {
                  "kube-aws.coreos.com/securitygroups": "'${SECURITY_GROUPS}'"
                }
              }
            }' \
         http://localhost:8080/api/v1/nodes/$(hostname)
  {{end -}}

{{ if .SharedPersistentVolume }}
  - path: /opt/bin/set-efs-pv
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/etc/kubernetes,readOnly=false \
        --mount volume=awsenv,target=/etc/kubernetes \
        --uuid-file-save=/var/run/coreos/set-efs-pv.uuid \
        --set-env={{.StackNameEnvVarName}}=${{.StackNameEnvVarName}} \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          '
           cfn-init -v -c "load-efs-pv" --region {{.Region}} --resource {{.Controller.LogicalName}} --stack "${{.StackNameEnvVarName}}"
          '

      rkt rm --uuid-file=/var/run/coreos/set-efs-pv.uuid || :
{{end}}
  - path: /opt/bin/cfn-signal
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash

      result=$(rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-signal.uuid \
        --set-env={{.StackNameEnvVarName}}=${{.StackNameEnvVarName}} \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          '
            cfn-signal -e 0 --region {{.Region}} --resource {{.Controller.LogicalName}} --stack "${{.StackNameEnvVarName}}"
          ')
      result_code=$?
      echo $result
      rkt rm --uuid-file=/var/run/coreos/cfn-signal.uuid || :

      # Filter out benign failures to signal stack
      if [ $result_code -ne 0 ]; then
        case $result in
          *"for resource Controllers already exists"*)     exit 0
            ;;
          *"is in UPDATE_COMPLETE state and cannot be"*)   exit 0
            ;;
        esac
        exit 1
      fi
      exit 0

  - path: /opt/bin/cfn-etcd-environment
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume=awsenv,kind=host,source=/var/run/coreos,readOnly=false \
        --mount volume=awsenv,target=/var/run/coreos \
        --uuid-file-save=/var/run/coreos/cfn-etcd-environment.uuid \
        --set-env={{.StackNameEnvVarName}}=${{.StackNameEnvVarName}} \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          '
            cfn-init -v -c "etcd-client" --region {{.Region}} --resource {{.Controller.LogicalName}} --stack "${{.StackNameEnvVarName}}"
          '

      rkt rm --uuid-file=/var/run/coreos/cfn-etcd-environment.uuid || :

  - path: /etc/default/kubelet
    permissions: 0755
    owner: root:root
    content: |
      KUBELET_OPTS="{{.Experimental.KubeletOpts}}"

  - path: /opt/bin/kubectl
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      echo >&2 "Running kubectl in ${PWD}"
      vols="-v /srv/kubernetes:/srv/kubernetes:ro -v /etc/kubernetes:/etc/kubernetes:ro -v ${PWD}:/work:rw"
      /usr/bin/docker run -i -t --rm -w /work $vols --net=host {{.HyperkubeImage.RepoWithTag}} /kubectl --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml $@

  - path: /opt/bin/install-kube-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      DEPLOY_MANIFEST_LIST=()
      DELETE_MANIFEST_LIST=()
      DELETE_REFERENCE_LIST=()

      vols="-v /srv/kubernetes:/srv/kubernetes:ro -v /etc/kubernetes:/etc/kubernetes:ro -v /srv/kube-aws:/srv/kube-aws:ro"
      mfdir=/srv/kubernetes/manifests
      rbac=/srv/kubernetes/rbac
      kubectl() {
          # --request-timeout=1s is intended to instruct kubectl to give up discovering unresponsive apiservice(s) in certain periods
          # so that temporal freakiness/unresponsity of specific apiservice until apiserver/controller-manager fully starts doesn't
          # affect the whole controller bootstrap process.
          /usr/bin/docker run -i --rm $vols --net=host {{.HyperkubeImage.RepoWithTag}} /kubectl --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml --request-timeout=1s "$@"
      }

      helm() {
        /usr/bin/docker run --rm --net=host \
          -v /etc/resolv.conf:/etc/resolv.conf \
          -v {{.HelmReleasePlugin.Directory}}:{{.HelmReleasePlugin.Directory}} \
          {{.HelmImage.RepoWithTag}} helm --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml "$@"
      }

      # Add manifests to the deployment list
      deploy() {
          until [ -z "$1" ]
          do
              echo "Adding '$1' to the deploy manifest list"
              DEPLOY_MANIFEST_LIST+=("$1")
              shift
          done
      }

      # Delete manifests - add manifests to the delete by manifest list
      remove() {
          until [ -z "$1" ]
          do
              echo "Adding '$1' to the delete manifest list"
              DELETE_MANIFEST_LIST+=("$1")
              shift
          done
      }

      # Add's lists of objectType:reference to the delete by reference list
      # This is useful for removing deprecated objects by name where we don't want to
      # keep the old manifests hanging around, e.g. ClusterRole:calico or deploy:kube-system/heapster
      remove_object() {
          local object=$1
          local reference=$2

          echo "Adding ${object} ${reference} to the delete by reference list"
          DELETE_REFERENCE_LIST+=("${object}:${reference}")
      }

      # Use --force to ensure immutable objects are deleted+added if can not be applied.
      apply_deploys() {
        kubectl apply --force -f $(echo "$@" | tr ' ' ',')
      }

      # Manage the deletion of services
      apply_manifest_deletes() {
        kubectl delete --cascade=true --ignore-not-found=true -f $(echo "$@" | tr ' ' ',')
      }

      # Deploy a list of helm charts
      apply_helm_deploys() {
        until [ -z "$1" ]
        do
          while read r || [[ -n $r ]]; do
            local release_name=$(jq .name $r)
            local chart_name=$(jq .chart.name $r)
            local chart_version=$(jq .chart.version $r)
            local values_file=$(jq .values.file $r)
            echo "Checking status of helm release $release_name"
            if helm status $release_name; then
              echo "Upgrading helm release ${release_name} of chart ${chart_name} with version ${chart_version} and values ${values_file}"
              helm upgrade $release_name $chart_name --version $chart_version -f $values_file
            else
              echo "Installing helm release ${release_name} of chart ${chart_name} with version ${chart_version} and values ${values_file}"
              helm install $release_name $chart_name --version $chart_version -f $values_file
            fi
          done <$1
        done
      }

      # Delete of the objects listed by reference object:name or object:namespace/name
      apply_object_deletes() {
          until [ -z "$1" ]
          do
              local object="${1%%:*}"
              local spaceAndName="${1##*:}"
              local name="${spaceAndName##*/}"
              local namespace
              if [[ "${name}" != "${spaceAndName}" ]]; then
                namespace="${spaceAndName%%/*}"
              fi

              if [[ -z "${namespace}" ]]; then
                echo "Executing delete of non-namespaced ${object} ${name}..."
                kubectl delete ${object} ${name} --cascade=true --ignore-not-found=true
              else
                echo "Executing delete of ${object} ${name} from namespace ${namespace}..."
                kubectl -n ${namespace} delete ${object} ${name} --cascade=true --ignore-not-found=true
              fi
              shift
          done
      }

      # forceapply - remove and retry if apply fails (does not rely on the kubectl --force method)
      # this is needed for allowing the updating of pod disruption budgets
      forceapply() {
        set +e
        if ! kubectl apply -f $(echo "$@" | tr ' ' ','); then
          set -e
          kubectl delete --ignore-not-found=true -f $(echo "$@" | tr ' ' ',')
          kubectl create -f $(echo "$@" | tr ' ' ',')
        fi
        set -e
      }

      count_objects() {
        if [[ -n "$2" ]]; then
          kubectl get $1 --ignore-not-found=true --no-headers=true $2 | wc -l
        else
          kubectl get $1 --ignore-not-found=true --no-headers=true | wc -l
        fi
      }

      while ! kubectl get ns kube-system; do
        echo Waiting until kube-system created.
        sleep 3
      done

      # KUBE_SYSTEM NAMESPACE
      deploy "/srv/kubernetes/manifests/kube-system-ns.yaml"

      # KUBE_PROXY
      deploy "${mfdir}/kube-proxy-sa.yaml" \
        "${mfdir}/kube-proxy-cm.yaml" \
        "${mfdir}/kube-proxy-ds.yaml"

      {{- if .Experimental.CloudControllerManager.Enabled }}
      # CLOUD_CONTROLLER_MANAGER
      deploy "${mfdir}/cloud-controller-manager.yaml"

      {{- end }}
      {{- if .Experimental.ContainerStorageInterface.Enabled }}
      # CSI
      deploy "${mfdir}/csi-controller.yaml"
      deploy "${mfdir}/csi-node.yaml"
      deploy "${mfdir}/csi-rbac.yaml"
      deploy "${mfdir}/csi-driver.yaml"

      {{- end }}
      # TLS BOOTSTRAP
      deploy "${rbac}/cluster-role-bindings"/{nodes-can-create-csrs,automatically-sign-node-certificate-requests,automatically-sign-node-certificate-renewals}".yaml"

      # General Cluster roles and bindings
      deploy "${rbac}/cluster-roles/"{node-extensions,node-access}".yaml" \
        "${rbac}/cluster-role-bindings"/{kube-admin,system-worker,node,node-proxier,node-extensions,node-access}".yaml"

      # CORE KUBE-AWS PSP
      # Ensure kube-system service accounts can create priviliged pods via PodSecurityPolicy
      # Ensure that nodes can create shadow pods for their manifest services
      deploy ${mfdir}/core-psp.yaml \
        ${rbac}/cluster-roles/core-psp.yaml \
        ${rbac}/role-bindings/core-psp.yaml \
        ${rbac}/cluster-role-bindings/core-psp-node.yaml

      # CLUSTER NETWORKING
      deploy "${rbac}/network-daemonsets.yaml"
      {{- if .Kubernetes.Networking.AmazonVPC.Enabled }}
      deploy "${mfdir}/aws-k8s-cni.yaml"
      remove "${mfdir}/flannel.yaml"
      remove "${mfdir}/canal.yaml"
      {{- else if eq .Kubernetes.Networking.SelfHosting.Type "canal" }}
      remove "${mfdir}/flannel.yaml"
      deploy "${mfdir}/canal.yaml"
      {{- else }}
      remove "${mfdir}/canal.yaml"
      deploy "${mfdir}/flannel.yaml"
      {{- end }}

      # CLUSTER DNS
      {{ if not .KubeDns.DNSMasq.CoreDNSLocal.Enabled -}}
      {{ if eq .KubeDns.Provider "coredns" -}}
      remove_object ConfigMap kube-system/kubedns-cm
      remove_object Deployment kube-system/kube-dns
      deploy "${mfdir}/coredns-sa.yaml" \
        "${mfdir}/coredns-cr.yaml" \
        "${mfdir}/coredns-crb.yaml" \
        "${mfdir}/coredns-cm.yaml" \
        "${mfdir}/kube-dns-svc.yaml" \
        "${mfdir}/kube-dns-autoscaler-de.yaml" \
        "${mfdir}/coredns-de.yaml"
      {{- else }}
      remove_object ConfigMap kube-system/coredns-cm
      remove_object Deployment kube-system/coredns
      deploy "${mfdir}/kube-dns-sa.yaml" \
        "${mfdir}/kube-dns-cm.yaml" \
        "${mfdir}/kube-dns-svc.yaml" \
        "${mfdir}/kube-dns-autoscaler-de.yaml" \
        "${mfdir}/kube-dns-de.yaml"
      {{- end }}
      {{- else }}
      remove_object ConfigMap kube-system/kubedns-cm
      remove_object Deployment kube-system/kube-dns
      remove_object ConfigMap kube-system/coredns-cm
      remove_object Deployment kube-system/coredns
      remove_object Deployment kube-system/kube-dns-autoscaler
      {{- end }}
      {{- if .KubeDns.NodeLocalResolver }}
      {{- if .KubeDns.DNSMasq.CoreDNSLocal.Enabled }}
      deploy "${mfdir}/dnsmasq-node-coredns-local.yaml"
      {{- else }}
      remove "${mfdir}/dnsmasq-node-coredns-local.yaml"
      {{- end }}
      deploy "${mfdir}/dnsmasq-node-ds.yaml"
      {{- end }}
      {{ if not .KubeDns.DNSMasq.CoreDNSLocal.Enabled -}}
      forceapply "${mfdir}/kube-dns-pdb.yaml"
      {{- else }}
      remove "${mfdir}/kube-dns-pdb.yaml"
      {{- end }}

      {{ if .Addons.MetricsServer.Enabled -}}
      # METRICS SERVER
      deploy \
        "${mfdir}/metrics-server-sa.yaml" \
        "${mfdir}/metrics-server-de.yaml" \
        "${mfdir}/metrics-server-svc.yaml" \
        "${rbac}/cluster-roles/metrics-server.yaml" \
        "${rbac}/cluster-role-bindings/metrics-server.yaml" \
        "${rbac}/role-bindings/metrics-server.yaml" \
        "${mfdir}/metrics-server-apisvc.yaml"

      {{- end }}
      {{ if .Addons.Rescheduler.Enabled -}}
      # RESCHEDULER
      deploy "${mfdir}/kube-rescheduler-de.yaml"

      {{ end -}}
      {{ if .KubeResourcesAutosave.Enabled -}}
      # AUTO-RESOURCE SAVER
      deploy "${mfdir}/kube-resources-autosave-de.yaml"

      {{ end -}}
      # HELM/TILLER
      deploy "${mfdir}/tiller-rbac.yaml" \
        "${mfdir}/tiller.yaml"

      {{ if .Experimental.NodeDrainer.Enabled -}}
      # NODE DRAINER
      deploy "${mfdir}/kube-node-drainer-ds.yaml" \
        "${mfdir}/kube-node-drainer-asg-status-updater-de.yaml"

      {{ end -}}
      {{ if .Experimental.GpuSupport.Enabled -}}
      # NVIDIA GPU SUPPORT
      deploy "${mfdir}/nvidia-driver-installer.yaml"

      {{ end -}}

      # CUSTOMER SUPPLIED MANIFESTS
      # Allow installing kubernetes manifests via customFiles in the controller config - installs all manifests in mfdir/custom directory.
      if ls ${mfdir}/custom/*.yaml &> /dev/null; then
        deploy ${mfdir}/custom/*.yaml
      fi

      {{ if .KubernetesManifestPlugin.ManifestListFile.Path -}}
      # PLUGIN SUPPLIED MANIFESTS
      if [[ -s {{.KubernetesManifestPlugin.ManifestListFile.Path}} ]]; then
        while read m || [[ -n $m ]]; do
          deploy $m
        done <{{.KubernetesManifestPlugin.ManifestListFile.Path}}
      fi

      {{- end }}
      # REMOVE LEGACY HEAPSTER
      remove_object ClusterRoleBinding heapster
      remove_object RoleBinding kube-system/heapster-nanny
      remove_object ConfigMap kube-system/heapster-config
      remove_object ServiceAccount kube-system/heapster
      remove_object Deployment kube-system/heapster
      remove_object Service kube-system/heapster

      # DEPLOY SELECTED MANIFESTS
      apply_manifest_deletes ${DELETE_MANIFEST_LIST[@]}
      apply_object_deletes ${DELETE_REFERENCE_LIST[@]}
      apply_deploys ${DEPLOY_MANIFEST_LIST[@]}

      {{ if .HelmReleasePlugin.ReleaseListFile.Path -}}
      # APPLY KUBE-AWS PLUGIN SUPPLIED HELM CHARTS
      if [[ -s {{.HelmReleasePlugin.ReleaseListFile.Path}} ]]; then
      apply_helm_deploys {{.HelmReleasePlugin.ReleaseListFile.Path}}
      fi

      {{- end }}
      # Check for the existence of any PodSecurityPolices after the system and plugins have been deployed.
      # Bind all serviceaccounts and authenitcated users to the kube-aws permissive policy is there are no other policies defined.
      if [[ "$(count_objects PodSecurityPolicy)" == "1" ]]; then
        echo "Only default kube-aws psp found: Binding all service accounts and authenticated users to this permissive policy"
        apply_deploys ${rbac}/cluster-role-bindings/default-permissive-psp.yaml
      fi

      echo "install-kube-system finished successfully :)"

  - path: /srv/kubernetes/manifests/core-psp.yaml
    content: |
      apiVersion: policy/v1beta1
      kind: PodSecurityPolicy
      metadata:
        name: 00-kube-aws-permissive
        annotations:
          seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      spec:
        privileged: true
        allowPrivilegeEscalation: true
        allowedCapabilities:
        - '*'
        volumes:
        - '*'
        hostNetwork: true
        hostPorts:
        - min: 0
          max: 65535
        hostIPC: true
        hostPID: true
        runAsUser:
          rule: 'RunAsAny'
        seLinux:
          rule: 'RunAsAny'
        supplementalGroups:
          rule: 'RunAsAny'
        fsGroup:
          rule: 'RunAsAny'

  - path: /srv/kubernetes/rbac/cluster-roles/core-psp.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: kube-aws:permissive-psp
      rules:
      - apiGroups: ['policy']
        resources: ['podsecuritypolicies']
        verbs:     ['use']
        resourceNames:
        - 00-kube-aws-permissive

  - path: /srv/kubernetes/rbac/role-bindings/core-psp.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: kube-aws:permissive-psp-kube-system
        namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: kube-aws:permissive-psp
        apiGroup: rbac.authorization.k8s.io
      subjects:
      - kind: Group
        name: system:serviceaccounts

  - path: /srv/kubernetes/rbac/cluster-role-bindings/core-psp-node.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: kube-aws:permissive-psp-nodes
      roleRef:
        kind: ClusterRole
        name: kube-aws:permissive-psp
        apiGroup: rbac.authorization.k8s.io
      subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: Group
        name: system:nodes
      - apiGroup: rbac.authorization.k8s.io
        kind: User
        name: kube-worker

  - path: /srv/kubernetes/rbac/cluster-role-bindings/default-permissive-psp.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: kube-aws:permissive-psp-cluster-wide
      roleRef:
        kind: ClusterRole
        name: kube-aws:permissive-psp
        apiGroup: rbac.authorization.k8s.io
      subjects:
      - kind: Group
        name: system:serviceaccounts
      - kind: Group
        name: system:autheniticated

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependencies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"

  - path: /srv/kubernetes/manifests/canal.yaml
    content: |
      # Based on https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml

      # This ConfigMap can be used to configure a self-hosted Canal installation.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: canal-config
        namespace: kube-system
      data:
        # The interface used by canal for host <-> host communication.
        # If left blank, then the interface is chosen using the node's
        # default route.
        canal_iface: ""

        # Whether or not to masquerade traffic to destinations not within
        # the pod network.
        masquerade: "true"

        # To enable Typha, set this to "calico-typha" *and* set a non-zero value for Typha replicas
        # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is
        # essential.
        {{ if .Kubernetes.Networking.SelfHosting.Typha -}}
        typha_service_name: "calico-typha"
        {{ else -}}
        typha_service_name: "none"
        {{- end }}

        # Configure the MTU to use
        veth_mtu: "1440"

        # The CNI network configuration to install on each node.
        cni_network_config: |-
          {
              "name": "k8s-pod-network",
              "cniVersion": "0.3.1",
              "plugins": [
                  {
                      "type": "calico",
                      "log_level": "info",
                      "mtu": 8951,
                      "datastore_type": "kubernetes",
                      "nodename": "__KUBERNETES_NODE_NAME__",
                      "ipam": {
                          "type": "host-local",
                          "subnet": "usePodCidr"
                      },
                      "policy": {
                          "type": "k8s",
                          "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                      },
                      "kubernetes": {
                          "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                          "kubeconfig": "__KUBECONFIG_FILEPATH__"
                      }
                  },
                  {
                      "type": "portmap",
                      "capabilities": {"portMappings": true},
                      "snat": true,
                      "externalSetMarkChain": "KUBE-MARK-MASQ"
                  }
              ]
          }

        # Flannel network configuration. Mounted into the flannel container.
        net-conf.json: |
          {
            "Network": "{{ .PodCIDR }}",
            {{ if gt .Kubernetes.Networking.SelfHosting.FlannelConfig.SubnetLen 0 -}}
            "SubnetLen": "{{ int (.Kubernetes.Networking.SelfHosting.FlannelConfig.SubnetLen ) }}",
            {{- end }}
            "Backend": {
              "Type": "vxlan"
            }
          }

{{- if .Kubernetes.Networking.SelfHosting.Typha }}
      ---
      # This manifest creates a Service, which will be backed by Calico's Typha daemon.
      # Typha sits in between Felix and the API server, reducing Calico's load on the API server.

      apiVersion: v1
      kind: Service
      metadata:
        name: calico-typha
        namespace: kube-system
        labels:
          k8s-app: calico-typha
      spec:
        ports:
          - port: 5473
            protocol: TCP
            targetPort: calico-typha
            name: calico-typha
        selector:
          k8s-app: calico-typha

      ---
      # This manifest creates a Deployment of Typha to back the above service.

      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: calico-typha
        namespace: kube-system
        labels:
          k8s-app: calico-typha
      spec:
        # Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the
        # typha_service_name variable in the calico-config ConfigMap above.
        #
        # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential
        # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In
        # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.
        replicas: 3
        selector:
          matchLabels:
            k8s-app: calico-typha
        revisionHistoryLimit: 2
        template:
          metadata:
            labels:
              k8s-app: calico-typha
            annotations:
              # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
              # add-on, ensuring it gets priority scheduling and that its resources are reserved
              # if it ever gets evicted.
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-cluster-critical
            tolerations:
            - key: CriticalAddonsOnly
              operator: Exists
            # Since Calico can't network a pod until Typha is up, we need to run Typha itself
            # as a host-networked pod.
            hostNetwork: true
            serviceAccountName: canal
            containers:
            - image: {{ .Kubernetes.Networking.SelfHosting.TyphaImage.RepoWithTag }}
              name: typha
              ports:
              - containerPort: 5473
                name: calico-typha
                protocol: TCP
              env:
                # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
                - name: TYPHA_LOGSEVERITYSCREEN
                  value: "info"
                # Disable logging to file and syslog since those don't make sense in Kubernetes.
                - name: TYPHA_LOGFILEPATH
                  value: "none"
                - name: TYPHA_LOGSEVERITYSYS
                  value: "none"
                # Monitor the Kubernetes API to find the number of running instances and rebalance
                # connections.
                - name: TYPHA_CONNECTIONREBALANCINGMODE
                  value: "kubernetes"
                - name: TYPHA_DATASTORETYPE
                  value: "kubernetes"
                - name: TYPHA_HEALTHENABLED
                  value: "true"
                # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
                # this opens a port on the host, which may need to be secured.
                #- name: TYPHA_PROMETHEUSMETRICSENABLED
                #  value: "true"
                #- name: TYPHA_PROMETHEUSMETRICSPORT
                #  value: "9093"
              resources:
                requests:
                  cpu: {{ .Kubernetes.Networking.SelfHosting.TyphaResources.Requests.Cpu }}
                  memory: {{ .Kubernetes.Networking.SelfHosting.TyphaResources.Requests.Memory }}
                limits:
                  cpu: {{ .Kubernetes.Networking.SelfHosting.TyphaResources.Limits.Cpu }}
                  memory: {{ .Kubernetes.Networking.SelfHosting.TyphaResources.Limits.Memory }}
              livenessProbe:
                httpGet:
                  path: /liveness
                  port: 9098
                  scheme: HTTP
                  host: 127.0.0.1
                periodSeconds: 30
                initialDelaySeconds: 30
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 9098
                  scheme: HTTP
                  host: 127.0.0.1
                periodSeconds: 10
{{- end }}
      # Canal Daemonset targeting masters/controllers
      # They will never use Typha to proxy the kube-apiserver.
      # Resolves Two bugs when Typha is enabled:
      # 1. Masters depend on Typha to start on the nodes for their networking.
      # 2. Typha will not start up without existing calico api objects.
      ---
      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: apps/v1
      metadata:
        name: canal-master
        namespace: kube-system
        labels:
          k8s-app: canal-master
      spec:
        selector:
          matchLabels:
            k8s-app: canal-master
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        template:
          metadata:
            labels:
              k8s-app: canal-master
              role.kubernetes.io/networking: "1"
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            # Ensure that canal-master only targets our masters.
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: node.kubernetes.io/role
                      operator: In
                      values:
                      - "master"
            hostNetwork: true
            serviceAccountName: canal
            tolerations:
              # Tolerate this effect so the pods will be schedulable at all times
              - effect: NoSchedule
                operator: Exists
              # Mark the pod as a critical add-on for rescheduling.
              - key: CriticalAddonsOnly
                operator: Exists
              - effect: NoExecute
                operator: Exists
            # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
            # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
            terminationGracePeriodSeconds: 0
            initContainers:
              # This container installs the CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: {{ .Kubernetes.Networking.SelfHosting.CalicoCniImage.RepoWithTag }}
                command: ["/install-cni.sh"]
                env:
                  - name: CNI_NET_DIR
                    value: /etc/kubernetes/cni/net.d
                  # Name of the CNI config file to create.
                  - name: CNI_CONF_NAME
                    value: "10-canal.conflist"
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: cni_network_config
                  # Set the hostname based on the k8s node name.
                  - name: KUBERNETES_NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  # CNI MTU Config variable
                  - name: CNI_MTU
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: veth_mtu
                  # Prevents the container from sleeping forever.
                  - name: SLEEP
                    value: "false"
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
                securityContext:
                  privileged: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: {{ .Kubernetes.Networking.SelfHosting.CalicoNodeImage.RepoWithTag }}
                env:
                  # Use Kubernetes API as the backing datastore.
                  - name: DATASTORE_TYPE
                    value: "kubernetes"
                  # Configure route aggregation based on pod CIDR.
                  - name: USE_POD_CIDR
                    value: "true"
                  # Enable felix logging.
                  - name: FELIX_LOGSEVERITYSYS
                    value: "Warning"
                  # Don't enable BGP.
                  - name: CALICO_NETWORKING_BACKEND
                    value: "none"
                  # Cluster type to identify the deployment type
                  - name: CLUSTER_TYPE
                    value: "k8s,canal"
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  # Period, in seconds, at which felix re-applies all iptables state
                  - name: FELIX_IPTABLESREFRESHINTERVAL
                    value: "60"
                  # Disable IPV6 support in Felix.
                  - name: FELIX_IPV6SUPPORT
                    value: "false"
                  # Wait for the datastore.
                  - name: WAIT_FOR_DATASTORE
                    value: "true"
                  # No IP address needed.
                  - name: IP
                    value: ""
                  # Typha support: is never enabled on masters
                  - name: FELIX_TYPHAK8SSERVICENAME
                    value: "none"
                  # Set MTU for tunnel device used if ipip is enabled
                  - name: FELIX_IPINIPMTU
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: veth_mtu
                  - name: NODENAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  # Set Felix endpoint to host default action to ACCEPT.
                  - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                    value: "ACCEPT"
                  - name: FELIX_HEALTHENABLED
                    value: "true"
                securityContext:
                  privileged: true
                resources:
                  requests:
                    cpu: 250m
                livenessProbe:
                  httpGet:
                    path: /liveness
                    port: 9099
                    scheme: HTTP
                    host: 127.0.0.1
                  periodSeconds: 10
                  initialDelaySeconds: 10
                  failureThreshold: 6
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 9099
                    scheme: HTTP
                    host: 127.0.0.1
                  periodSeconds: 10
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /run/xtables.lock
                    name: xtables-lock
                    readOnly: false
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /var/lib/calico
                    name: var-lib-calico
                    readOnly: false
                  - name: policysync
                    mountPath: /var/run/nodeagent
              # This container runs flannel using the kube-subnet-mgr backend
              # for allocating subnets.
              - name: flannel
                image: {{ .Kubernetes.Networking.SelfHosting.FlannelImage.RepoWithTag }}
                command: [ "/opt/bin/flanneld", "--ip-masq", "--kube-subnet-mgr" ]
                securityContext:
                  privileged: true
                env:
                  - name: FLANNELD_IPTABLES_FORWARD_RULES
                    value: "false"
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  - name: FLANNELD_IFACE
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: canal_iface
                  - name: FLANNELD_IP_MASQ
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: masquerade
                volumeMounts:
                - mountPath: /run/xtables.lock
                  name: xtables-lock
                  readOnly: false
                - name: flannel-cfg
                  mountPath: /etc/kube-flannel/
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              - name: var-lib-calico
                hostPath:
                  path: /var/lib/calico
              - name: xtables-lock
                hostPath:
                  path: /run/xtables.lock
                  type: FileOrCreate
              # Used by flannel.
              - name: flannel-cfg
                configMap:
                  name: canal-config
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              # Used to create per-pod Unix Domain Sockets
              - name: policysync
                hostPath:
                  type: DirectoryOrCreate
                  path: /var/run/nodeagent

      # Canal DaemonSet for Nodes - Typha can be enabled.
      ---
      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: apps/v1
      metadata:
        name: canal-node
        namespace: kube-system
        labels:
          k8s-app: canal-node
          role.kubernetes.io/networking: "1"
      spec:
        selector:
          matchLabels:
            k8s-app: canal-node
        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 100%
        template:
          metadata:
            labels:
              k8s-app: canal-node
              role.kubernetes.io/networking: "1"
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            # Ensure that canal-node only targets nodes and not masters
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: node.kubernetes.io/role
                      operator: NotIn
                      values:
                      - "master"
            hostNetwork: true
            serviceAccountName: canal
            tolerations:
              # Tolerate this effect so the pods will be schedulable at all times
              - effect: NoSchedule
                operator: Exists
              # Mark the pod as a critical add-on for rescheduling.
              - key: CriticalAddonsOnly
                operator: Exists
              - effect: NoExecute
                operator: Exists
            # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
            # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
            terminationGracePeriodSeconds: 0
            initContainers:
              # This container installs the CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: {{ .Kubernetes.Networking.SelfHosting.CalicoCniImage.RepoWithTag }}
                command: ["/install-cni.sh"]
                env:
                  - name: CNI_NET_DIR
                    value: /etc/kubernetes/cni/net.d
                  # Name of the CNI config file to create.
                  - name: CNI_CONF_NAME
                    value: "10-canal.conflist"
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: cni_network_config
                  # Set the hostname based on the k8s node name.
                  - name: KUBERNETES_NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  # CNI MTU Config variable
                  - name: CNI_MTU
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: veth_mtu
                  # Prevents the container from sleeping forever.
                  - name: SLEEP
                    value: "false"
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
                securityContext:
                  privileged: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: {{ .Kubernetes.Networking.SelfHosting.CalicoNodeImage.RepoWithTag }}
                env:
                  # Use Kubernetes API as the backing datastore.
                  - name: DATASTORE_TYPE
                    value: "kubernetes"
                  # Configure route aggregation based on pod CIDR.
                  - name: USE_POD_CIDR
                    value: "true"
                  # Enable felix logging.
                  - name: FELIX_LOGSEVERITYSYS
                    value: "Warning"
                  # Don't enable BGP.
                  - name: CALICO_NETWORKING_BACKEND
                    value: "none"
                  # Cluster type to identify the deployment type
                  - name: CLUSTER_TYPE
                    value: "k8s,canal"
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  # Period, in seconds, at which felix re-applies all iptables state
                  - name: FELIX_IPTABLESREFRESHINTERVAL
                    value: "60"
                  # Disable IPV6 support in Felix.
                  - name: FELIX_IPV6SUPPORT
                    value: "false"
                  # Wait for the datastore.
                  - name: WAIT_FOR_DATASTORE
                    value: "true"
                  # No IP address needed.
                  - name: IP
                    value: ""
                  # Set MTU for tunnel device used if ipip is enabled
                  - name: FELIX_IPINIPMTU
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: veth_mtu
                  # Typha support: controlled by the ConfigMap.
                  - name: FELIX_TYPHAK8SSERVICENAME
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: typha_service_name
                  - name: NODENAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  # Set Felix endpoint to host default action to ACCEPT.
                  - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                    value: "ACCEPT"
                  - name: FELIX_HEALTHENABLED
                    value: "true"
                securityContext:
                  privileged: true
                resources:
                  requests:
                    cpu: 250m
                livenessProbe:
                  httpGet:
                    path: /liveness
                    port: 9099
                    scheme: HTTP
                    host: 127.0.0.1
                  periodSeconds: 10
                  initialDelaySeconds: 10
                  failureThreshold: 6
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 9099
                    scheme: HTTP
                    host: 127.0.0.1
                  periodSeconds: 10
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /run/xtables.lock
                    name: xtables-lock
                    readOnly: false
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /var/lib/calico
                    name: var-lib-calico
                    readOnly: false
                  - name: policysync
                    mountPath: /var/run/nodeagent
              # This container runs flannel using the kube-subnet-mgr backend
              # for allocating subnets.
              - name: flannel
                image: {{ .Kubernetes.Networking.SelfHosting.FlannelImage.RepoWithTag }}
                command: [ "/opt/bin/flanneld", "--ip-masq", "--kube-subnet-mgr" ]
                securityContext:
                  privileged: true
                env:
                  - name: FLANNELD_IPTABLES_FORWARD_RULES
                    value: "false"
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  - name: FLANNELD_IFACE
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: canal_iface
                  - name: FLANNELD_IP_MASQ
                    valueFrom:
                      configMapKeyRef:
                        name: canal-config
                        key: masquerade
                volumeMounts:
                - mountPath: /run/xtables.lock
                  name: xtables-lock
                  readOnly: false
                - name: flannel-cfg
                  mountPath: /etc/kube-flannel/
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              - name: var-lib-calico
                hostPath:
                  path: /var/lib/calico
              - name: xtables-lock
                hostPath:
                  path: /run/xtables.lock
                  type: FileOrCreate
              # Used by flannel.
              - name: flannel-cfg
                configMap:
                  name: canal-config
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              # Used to create per-pod Unix Domain Sockets
              - name: policysync
                hostPath:
                  type: DirectoryOrCreate
                  path: /var/run/nodeagent
      ---
      # Source: calico/templates/kdd-crds.yaml
      # Create all the CustomResourceDefinitions needed for
      # Calico policy and networking mode.
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: felixconfigurations.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: FelixConfiguration
          plural: felixconfigurations
          singular: felixconfiguration
      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: ipamblocks.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: IPAMBlock
          plural: ipamblocks
          singular: ipamblock

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: blockaffinities.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: BlockAffinity
          plural: blockaffinities
          singular: blockaffinity

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: ipamhandles.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: IPAMHandle
          plural: ipamhandles
          singular: ipamhandle

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: ipamconfigs.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: IPAMConfig
          plural: ipamconfigs
          singular: ipamconfig

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: bgppeers.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: BGPPeer
          plural: bgppeers
          singular: bgppeer

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: bgpconfigurations.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: BGPConfiguration
          plural: bgpconfigurations
          singular: bgpconfiguration

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: ippools.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: IPPool
          plural: ippools
          singular: ippool

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: hostendpoints.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: HostEndpoint
          plural: hostendpoints
          singular: hostendpoint

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: clusterinformations.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: ClusterInformation
          plural: clusterinformations
          singular: clusterinformation

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: globalnetworkpolicies.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: GlobalNetworkPolicy
          plural: globalnetworkpolicies
          singular: globalnetworkpolicy

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: globalnetworksets.crd.projectcalico.org
      spec:
        scope: Cluster
        group: crd.projectcalico.org
        version: v1
        names:
          kind: GlobalNetworkSet
          plural: globalnetworksets
          singular: globalnetworkset

      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: networkpolicies.crd.projectcalico.org
      spec:
        scope: Namespaced
        group: crd.projectcalico.org
        version: v1
        names:
          kind: NetworkPolicy
          plural: networkpolicies
          singular: networkpolicy
      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: networksets.crd.projectcalico.org
      spec:
        scope: Namespaced
        group: crd.projectcalico.org
        version: v1
        names:
          kind: NetworkSet
          plural: networksets
          singular: networkset

  - path: /srv/kubernetes/rbac/network-daemonsets.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: canal
        namespace: kube-system

      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: flannel
        namespace: kube-system

      ---
      # Source: calico/templates/rbac.yaml
      # Include a clusterrole for the calico-node DaemonSet,
      # and bind it to the calico-node serviceaccount.
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: calico-node
      rules:
        # The CNI plugin needs to get pods, nodes, and namespaces.
        - apiGroups: [""]
          resources:
            - pods
            - nodes
            - namespaces
          verbs:
            - get
        - apiGroups: [""]
          resources:
            - endpoints
            - services
          verbs:
            # Used to discover service IPs for advertisement.
            - watch
            - list
            # Used to discover Typhas.
            - get
        - apiGroups: [""]
          resources:
            - nodes/status
          verbs:
            # Needed for clearing NodeNetworkUnavailable flag.
            - patch
            # Calico stores some configuration information in node annotations.
            - update
        # Watch for changes to Kubernetes NetworkPolicies.
        - apiGroups: ["networking.k8s.io"]
          resources:
            - networkpolicies
          verbs:
            - watch
            - list
        # Used by Calico for policy information.
        - apiGroups: [""]
          resources:
            - pods
            - namespaces
            - serviceaccounts
          verbs:
            - list
            - watch
        # The CNI plugin patches pods/status.
        - apiGroups: [""]
          resources:
            - pods/status
          verbs:
            - patch
        # Calico monitors various CRDs for config.
        - apiGroups: ["crd.projectcalico.org"]
          resources:
            - globalfelixconfigs
            - felixconfigurations
            - bgppeers
            - globalbgpconfigs
            - bgpconfigurations
            - blockaffinities
            - ippools
            - ipamblocks
            - ipamhandles
            - ipamconfigs
            - globalnetworkpolicies
            - globalnetworksets
            - networkpolicies
            - networksets
            - clusterinformations
            - hostendpoints
          verbs:
            - get
            - list
            - watch
        # Calico must create and update some CRDs on startup.
        - apiGroups: ["crd.projectcalico.org"]
          resources:
            - ippools
            - felixconfigurations
            - clusterinformations
          verbs:
            - create
            - update
        # Calico stores some configuration information on the node.
        - apiGroups: [""]
          resources:
            - nodes
          verbs:
            - get
            - list
            - watch
        # These permissions are only requried for upgrade from v2.6, and can
        # be removed after upgrade or on fresh installations.
        - apiGroups: ["crd.projectcalico.org"]
          resources:
            - bgpconfigurations
            - bgppeers
          verbs:
            - create
            - update

      ---
      # Flannel roles
      # Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: flannel
      rules:
        - apiGroups:
            - ""
          resources:
            - pods
          verbs:
            - get
        - apiGroups:
            - ""
          resources:
            - nodes
          verbs:
            - list
            - watch
        - apiGroups:
            - ""
          resources:
            - nodes/status
          verbs:
            - patch

      ---
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: flannel
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: flannel
      subjects:
      - kind: ServiceAccount
        name: flannel
        namespace: kube-system

      ---
      # Bind the flannel ClusterRole to the canal ServiceAccount.
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: canal-flannel
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: flannel
      subjects:
      - kind: ServiceAccount
        name: canal
        namespace: kube-system

      ---
      # Bind the calico ClusterRole to the canal ServiceAccount.
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: canal-calico
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: calico-node
      subjects:
      - kind: ServiceAccount
        name: canal
        namespace: kube-system

  - path: /srv/kubernetes/manifests/flannel.yaml
    content: |
      ---
      # Keep cni network name 'calico' to avoid ip clashes when switching to flannel.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: flannel-cfg
        namespace: kube-system
        labels:
          tier: node
          app: flannel
      data:
        cni-conf.json: |
          {
            "name": "calico",
            "plugins": [
              {
                "type": "flannel",
                "delegate": {
                  "hairpinMode": true,
                  "isDefaultGateway": true
                },
                "ipam": {
                  "type": "host-local",
                  "subnet": "usePodCidr"
                }
              },
              {
                "type": "portmap",
                "capabilities": {
                  "portMappings": true
                }
              }
            ]
          }
        net-conf.json: |
          {
            "Network": "{{ .PodCIDR }}",
             {{ if gt .Kubernetes.Networking.SelfHosting.FlannelConfig.SubnetLen 0 -}}
            "SubnetLen": "{{ int (.Kubernetes.Networking.SelfHosting.FlannelConfig.SubnetLen) }}",
            {{- end }}
            "Backend": {
              "Type": "vxlan"
            }
          }
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: flannel
        namespace: kube-system
        labels:
          tier: node
          app: flannel
      spec:
        selector:
          matchLabels:
            tier: node
            app: flannel
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        template:
          metadata:
            labels:
              tier: node
              app: flannel
          spec:
            priorityClassName: system-node-critical
            hostNetwork: true
            nodeSelector:
              beta.kubernetes.io/arch: amd64
            tolerations:
              # Tolerate this effect so the pods will be schedulable at all times
              - effect: NoSchedule
                operator: Exists
              # Mark the pod as a critical add-on for rescheduling.
              - key: CriticalAddonsOnly
                operator: Exists
              - effect: NoExecute
                operator: Exists
            serviceAccountName: flannel
            # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
            # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
            terminationGracePeriodSeconds: 0
            initContainers:
              - name: remove-cni-networks
                image: {{.HyperkubeImage.RepoWithTag}}
                command:
                - /bin/rm
                - -rf
                - /etc/kubernetes/cni/net.d/10-calico.conflist
                - /etc/kubernetes/cni/net.d/10-calico.conf
                volumeMounts:
                - mountPath: /etc/kubernetes/cni/net.d
                  name: cni-net-dir
            containers:
            # This container installs the Flannel CNI binaries
            # and CNI network config file on each node.
            - name: install-cni
              image: {{ .Kubernetes.Networking.SelfHosting.FlannelCniImage.RepoWithTag }}
              command: ["/install-cni.sh"]
              env:
                # The CNI network config to install on each node.
                - name: CNI_NETWORK_CONFIG
                  valueFrom:
                    configMapKeyRef:
                      name: flannel-cfg
                      key: cni-conf.json
              volumeMounts:
                - mountPath: /host/opt/cni/bin
                  name: cni-bin-dir
                - mountPath: /host/etc/cni/net.d
                  name: cni-net-dir
            - name: flannel
              image: {{ .Kubernetes.Networking.SelfHosting.FlannelImage.RepoWithTag }}
              command:
              - /opt/bin/flanneld
              args:
              - --ip-masq
              - --kube-subnet-mgr
              resources:
                requests:
                  cpu: "100m"
                  memory: "50Mi"
                limits:
                  cpu: "100m"
                  memory: "50Mi"
              securityContext:
                privileged: true
              env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              volumeMounts:
              - name: run
                mountPath: /run
              - name: flannel-cfg
                mountPath: /etc/kube-flannel/
            volumes:
              - name: run
                hostPath:
                  path: /run
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: flannel-cfg
                configMap:
                  name: flannel-cfg

{{ if .KubeResourcesAutosave.Enabled }}
  - path: /srv/kubernetes/manifests/kube-resources-autosave-de.yaml
    content: |
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kube-resources-autosave
        namespace: kube-system
        labels:
          k8s-app: kube-resources-autosave-policy
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: kube-resources-autosave-policy
        template:
          metadata:
            {{if (index .PluginConfigs "kiam").Enabled -}}
            annotations:
              iam.amazonaws.com/role: "{{$.ClusterName}}-IAMRoleResourcesAutoSave"
            {{ end -}}
            name: kube-resources-autosave
            namespace: kube-system
            labels:
              k8s-app: kube-resources-autosave-policy
          spec:
            priorityClassName: system-cluster-critical
            containers:
            - name: kube-resources-autosave-dumper
              image: {{.HyperkubeImage.RepoWithTag}}
              command: ["/bin/sh", "-c" ]
              args:
                - |
                    set -x ;
                    DUMP_DIR_COMPLETE=/kube-resources-autosave/complete ;
                    aws configure set s3.signature_version s3v4 ;
                    mkdir -p ${DUMP_DIR_COMPLETE} ;
                    while true; do
                      TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)
                      DUMP_DIR=/kube-resources-autosave/tmp/${TIMESTAMP} ;
                      mkdir -p ${DUMP_DIR} ;
                      RESOURCES_OUT_NAMESPACE="namespaces persistentvolumes nodes storageclasses clusterrolebindings clusterroles";
                      for r in ${RESOURCES_OUT_NAMESPACE};do
                        echo " Searching for resources: ${r}" ;
                        /kubectl get --export -o=json ${r} | \
                        jq '.items |= ([ .[] |
                            del(.status,
                            .metadata.uid,
                            .metadata.selfLink,
                            .metadata.resourceVersion,
                            .metadata.creationTimestamp,
                            .metadata.generation,
                            .spec.claimRef
                          )])' > ${DUMP_DIR}/${r}.json ;
                      done ;
                      RESOURCES_IN_NAMESPACE="componentstatuses configmaps daemonsets deployments endpoints events horizontalpodautoscalers
                      ingresses jobs limitranges networkpolicies  persistentvolumeclaims pods podsecuritypolicies podtemplates replicasets
                      replicationcontrollers resourcequotas secrets serviceaccounts services statefulsets customresourcedefinitions
                      poddisruptionbudgets roles rolebindings";
                      for ns in $(jq -r '.items[].metadata.name' < ${DUMP_DIR}/namespaces.json);do
                        echo "Searching in namespace: ${ns}" ;
                        mkdir -p ${DUMP_DIR}/${ns} ;
                        for r in ${RESOURCES_IN_NAMESPACE};do
                          echo " Searching for resources: ${r}" ;
                          /kubectl --namespace=${ns} get --export -o=json ${r} | \
                          jq '.items |= ([ .[] |
                            select(.type!="kubernetes.io/service-account-token") |
                            del(
                              .spec.clusterIP,
                              .metadata.uid,
                              .metadata.selfLink,
                              .metadata.resourceVersion,
                              .metadata.creationTimestamp,
                              .metadata.generation,
                              .metadata.annotations."pv.kubernetes.io/bind-completed",
                              .status
                            )])' > ${DUMP_DIR}/${ns}/${r}.json && touch /probe-token ;
                        done ;
                      done ;
                    mv ${DUMP_DIR} ${DUMP_DIR_COMPLETE}/${TIMESTAMP} ;
                    rm -r -f ${DUMP_DIR} ;
                    sleep 24h ;
                    done
              livenessProbe:
                exec:
                  command: ["/bin/sh", "-c",  "AGE=$(( $(date +%s) - $(stat -c%Y /probe-token) < 25*60*60 ));  [ $AGE -gt 0 ]" ]
                initialDelaySeconds: 240
                periodSeconds: 10
              volumeMounts:
              - name: dump-dir
                mountPath: /kube-resources-autosave
                readOnly: false
            - name: kube-resources-autosave-pusher
              image: {{.AWSCliImage.RepoWithTag}}
              command: ["/bin/sh", "-c" ]
              args:
                - |
                    set -x ;
                    DUMP_DIR_COMPLETE=/kube-resources-autosave/complete ;
                    while true; do
                      for FILE in ${DUMP_DIR_COMPLETE}/* ; do
                        aws s3 mv ${FILE} s3://{{ .KubeResourcesAutosave.S3Path }}/$(basename ${FILE}) --recursive && rm -r -f ${FILE} && touch /probe-token ;
                      done ;
                      sleep 1m ;
                    done
              livenessProbe:
                exec:
                  command: ["/bin/sh", "-c",  "AGE=$(( $(date +%s) - $(stat -c%Y /probe-token) < 25*60*60 ));  [ $AGE -gt 0 ]" ]
                initialDelaySeconds: 240
                periodSeconds: 10
              volumeMounts:
              - name: dump-dir
                mountPath: /kube-resources-autosave
                readOnly: false
            volumes:
            - name: dump-dir
              emptyDir: {}
{{ end }}

{{if .AssetsEncryptionEnabled }}
  - path: /opt/bin/decrypt-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      rkt run \
        --volume=etc-kube,kind=host,source=/etc/kubernetes,readOnly=false \
        --mount=volume=etc-kube,target=/etc/kubernetes \
        --volume=srv-kube,kind=host,source=/srv/kubernetes,readOnly=false \
        --mount=volume=srv-kube,target=/srv/kubernetes \
        --uuid-file-save=/var/run/coreos/decrypt-assets.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImage.Options}}{{.AWSCliImage.RktRepo}} --exec=/bin/bash -- \
          -ec \
          'echo decrypting assets
           shopt -s nullglob
           set -o pipefail
           for encKey in /etc/kubernetes/{ssl,additional-configs,auth}/*.enc; do
             if [ ! -f $encKey ]; then
               echo skipping non-existent file: $encKey 1>&2
               continue
             fi
             echo decrypting $encKey
             f=$(mktemp $encKey.XXXXXXXX)
             /usr/bin/aws \
               --region {{.Region}} kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 -d > $f
             mv -f $f ${encKey%.enc}
           done;

           authDir=/etc/kubernetes/auth
           echo generating $authDir/tokens.csv
           echo > $authDir/tokens.csv

           echo "injecting token into tokens.csv and the kubelet bootstrap kubeconfig file"
           bootstrap_token=$(cat /etc/kubernetes/auth/kubelet-tls-bootstrap-token.tmp)
           echo "${bootstrap_token},kubelet-bootstrap,10001,system:bootstrappers" >> $authDir/tokens.csv
           {{- if checkVersion ">= 1.14" .K8sVer }}
           sed -i -e "s#\$KUBELET_BOOTSTRAP_TOKEN#${bootstrap_token}#g" /etc/kubernetes/kubeconfig/worker-bootstrap.yaml
           {{- end }}
           {{- if .AssetsConfig.HasAuthTokens }}
           cat $authDir/tokens.csv.tmp >> $authDir/tokens.csv
           {{- end }}

           {{ if .Controller.CustomFiles -}}
           {{ range $i, $f := .Controller.CustomFiles -}}
           {{ if $f.Encrypted -}}
           encKey={{ $f.Path }}.enc
           if [ -f $encKey ]; then
             echo decrypting $encKey
             f=$(mktemp $encKey.XXXXXXXX)
             /usr/bin/aws \
               --region {{$.Region}} kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 -d > $f
             mv -f $f {{ $f.Path }}
             chmod {{ $f.PermissionsString }} {{ $f.Path }}
           fi
           {{ end -}}
           {{ end -}}
           {{ end -}}

           echo done.'

      rkt rm --uuid-file=/var/run/coreos/decrypt-assets.uuid || :
{{ end }}

{{if .Experimental.NodeDrainer.Enabled}}
  - path: /srv/kubernetes/manifests/kube-node-drainer-asg-status-updater-de.yaml
    content: |
      kind: Deployment
      apiVersion: apps/v1
      metadata:
        name: kube-node-drainer-asg-status-updater
        namespace: kube-system
        labels:
          k8s-app: kube-node-drainer-asg-status-updater
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: kube-node-drainer-asg-status-updater
        template:
          metadata:
            labels:
              k8s-app: kube-node-drainer-asg-status-updater
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              {{if ne .Experimental.NodeDrainer.IAMRole.ARN.Arn "" -}}
              iam.amazonaws.com/role: {{ .Experimental.NodeDrainer.IAMRole.ARN.Arn }}
              {{ end }}
          spec:
            priorityClassName: system-node-critical
            initContainers:
              - name: hyperkube
                image: {{.HyperkubeImage.RepoWithTag}}
                command:
                - /bin/cp
                - -f
                - /hyperkube
                - /workdir/kubectl
                volumeMounts:
                - mountPath: /workdir
                  name: workdir
            containers:
              - name: kube-node-drainer-asg-status-updater
                image: {{.AWSCliImage.RepoWithTag}}
                env:
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                command:
                - /bin/sh
                - -xec
                - |
                  metadata() { curl -s -S -f http://169.254.169.254/2016-09-02/"$1"; }
                  asg()      { aws --region="${REGION}" autoscaling "$@"; }

                  # Hyperkube binary is not statically linked, so we need to use
                  # the musl interpreter to be able to run it in this image
                  # See: https://github.com/kubernetes-incubator/kube-aws/pull/674#discussion_r118889687
                  kubectl() { /lib/ld-musl-x86_64.so.1 /opt/bin/kubectl "$@"; }

                  REGION=$(metadata dynamic/instance-identity/document | jq -r .region)
                  [ -n "${REGION}" ]

                  # Not customizable, for now
                  POLL_INTERVAL=10

                  # Keeps a comma-separated list of instances that need to be drained. Sets '-'
                  # to force the ConfigMap to be updated in the first iteration.
                  instances_to_drain='-'

                  # Instance termination detection loop
                  while sleep ${POLL_INTERVAL}; do

                    # Fetch the list of instances being terminated by their respective ASGs
                    updated_instances_to_drain=$(asg describe-auto-scaling-groups | jq -r '[.AutoScalingGroups[] | select((.Tags[].Key | contains("kube-aws:")) and (.Tags[].Key | contains("kubernetes.io/cluster/{{.ClusterName}}"))) | .Instances[] | select(.LifecycleState == "Terminating:Wait") | .InstanceId] | sort | join(",")')

                    # Have things changed since last iteration?
                    if [ "${updated_instances_to_drain}" == "${instances_to_drain}" ]; then
                      continue
                    fi
                    instances_to_drain="${updated_instances_to_drain}"

                    # Update ConfigMap to reflect current ASG state
                    echo "{\"apiVersion\": \"v1\", \"kind\": \"ConfigMap\", \"metadata\": {\"name\": \"kube-node-drainer-status\"}, \"data\": {\"asg\": \"${instances_to_drain}\"}}" | kubectl -n kube-system apply -f -
                  done
                volumeMounts:
                - mountPath: /opt/bin
                  name: workdir
            volumes:
              - name: workdir
                emptyDir: {}

  - path: /srv/kubernetes/manifests/kube-node-drainer-ds.yaml
    content: |
      kind: DaemonSet
      apiVersion: apps/v1
      metadata:
        name: kube-node-drainer-ds
        namespace: kube-system
        labels:
          k8s-app: kube-node-drainer-ds
      spec:
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        selector:
          matchLabels:
            k8s-app: kube-node-drainer-ds
        template:
          metadata:
            labels:
              k8s-app: kube-node-drainer-ds
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            tolerations:
            - operator: Exists
              effect: NoSchedule
            - operator: Exists
              effect: NoExecute
            - operator: Exists
              key: CriticalAddonsOnly
            initContainers:
              - name: hyperkube
                image: {{.HyperkubeImage.RepoWithTag}}
                command:
                - /bin/cp
                - -f
                - /hyperkube
                - /workdir/kubectl
                volumeMounts:
                - mountPath: /workdir
                  name: workdir
            containers:
              - name: kube-node-drainer
                image: {{.AWSCliImage.RepoWithTag}}
                env:
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                command:
                - /bin/sh
                - -xec
                - |
                  metadata() { curl -s -S -f http://169.254.169.254/2016-09-02/"$1"; }
                  asg()      { aws --region="${REGION}" autoscaling "$@"; }

                  # Hyperkube binary is not statically linked, so we need to use
                  # the musl interpreter to be able to run it in this image
                  # See: https://github.com/kubernetes-incubator/kube-aws/pull/674#discussion_r118889687
                  kubectl() { /lib/ld-musl-x86_64.so.1 /opt/bin/kubectl "$@"; }

                  INSTANCE_ID=$(metadata meta-data/instance-id)
                  REGION=$(metadata dynamic/instance-identity/document | jq -r .region)
                  [ -n "${REGION}" ]

                  # Not customizable, for now
                  POLL_INTERVAL=10

                  # Used to identify the source which requested the instance termination
                  termination_source=''

                  # Instance termination detection loop
                  while sleep ${POLL_INTERVAL}; do

                    # Spot instance termination check
                    http_status=$(curl -o /dev/null -w '%{http_code}' -sL http://169.254.169.254/latest/meta-data/spot/termination-time)
                    if [ "${http_status}" -eq 200 ]; then
                      termination_source=spot
                      break
                    fi

                    # Termination ConfigMap check
                    if [ -e /etc/kube-node-drainer/asg ] && grep -q "${INSTANCE_ID}" /etc/kube-node-drainer/asg; then
                      termination_source=asg
                      break
                    fi
                  done

                  # Node draining loop
                  while true; do
                    echo Node is terminating, draining it...

                    if ! kubectl drain --ignore-daemonsets=true --delete-local-data=true --force=true --timeout=60s "${NODE_NAME}"; then
                      echo Not all pods on this host can be evicted, will try again
                      continue
                    fi
                    echo All evictable pods are gone

                    if [ "${termination_source}" == asg ]; then
                      echo Notifying AutoScalingGroup that instance ${INSTANCE_ID} can be shutdown
                      ASG_NAME=$(asg describe-auto-scaling-instances --instance-ids "${INSTANCE_ID}" | jq -r '.AutoScalingInstances[].AutoScalingGroupName')
                      HOOK_NAME=$(asg describe-lifecycle-hooks --auto-scaling-group-name "${ASG_NAME}" | jq -r '.LifecycleHooks[].LifecycleHookName' | grep -i nodedrainer)
                      asg complete-lifecycle-action --lifecycle-action-result CONTINUE --instance-id "${INSTANCE_ID}" --lifecycle-hook-name "${HOOK_NAME}" --auto-scaling-group-name "${ASG_NAME}"
                    fi

                    # Expect instance will be shut down in defined drain timeout
                    sleep {{.Experimental.NodeDrainer.DrainTimeoutInSeconds}}
                  done
                volumeMounts:
                - mountPath: /opt/bin
                  name: workdir
                - mountPath: /etc/kube-node-drainer
                  name: kube-node-drainer-status
                  readOnly: true
            volumes:
            - name: workdir
              emptyDir: {}
            - name: kube-node-drainer-status
              projected:
                sources:
                - configMap:
                    name: kube-node-drainer-status
                    optional: true
{{end}}

  # TODO: remove the following binding once the TLS Bootstrapping feature is enabled by default, see:
  # https://github.com/kubernetes-incubator/kube-aws/pull/618#discussion_r115162048
  # https://kubernetes.io/docs/admin/authorization/rbac/#core-component-roles

  # Makes kube-worker user behave like a regular member of system:nodes group,
  # needed when TLS bootstrapping is disabled
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: kube-aws:node
      subjects:
        - kind: User
          name: kube-worker
      roleRef:
        kind: ClusterRole
        name: system:node
        apiGroup: rbac.authorization.k8s.io

  # We need to give nodes a few extra permissions so that both the node
  # draining and node labeling with AWS metadata work as expected
  - path: /srv/kubernetes/rbac/cluster-roles/node-extensions.yaml
    content: |
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
          name: kube-aws:node-extensions
      rules:
        - apiGroups: ["extensions"]
          resources:
          - daemonsets
          verbs:
          - get
        # Can be removed if node authorizer is enabled
        - apiGroups: [""]
          resources:
          - nodes
          verbs:
          - patch
          - update
        - apiGroups: ["extensions"]
          resources:
          - replicasets
          verbs:
          - get
        - apiGroups: ["batch"]
          resources:
          - jobs
          verbs:
          - get
        - apiGroups: [""]
          resources:
          - replicationcontrollers
          verbs:
          - get
        - apiGroups: [""]
          resources:
          - pods/eviction
          verbs:
          - create
        - nonResourceURLs: ["*"]
          verbs: ["*"]

  # Grants super-user permissions to the kube-admin user
  - path: /srv/kubernetes/rbac/cluster-role-bindings/kube-admin.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: kube-aws:admin
      subjects:
        - kind: User
          name: kube-admin
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: rbac.authorization.k8s.io

  # Also allows `kube-worker` user to perform actions needed by the
  # `kube-proxy` component.
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node-proxier.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: kube-aws:node-proxier
      subjects:
        - kind: User
          name: kube-worker
        - kind: ServiceAccount
          name: kube-proxy
          namespace: kube-system
        # Not needed after migrating to DaemonSet-based kube-proxy
        - kind: Group
          name: system:nodes
      roleRef:
        kind: ClusterRole
        name: system:node-proxier
        apiGroup: rbac.authorization.k8s.io

  # Allows add-ons running with the default service account in kube-sytem to have super-user access
  - path: /srv/kubernetes/rbac/cluster-role-bindings/system-worker.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: kube-aws:system-worker
      subjects:
        - kind: ServiceAccount
          namespace: kube-system
          name: default
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: rbac.authorization.k8s.io

  # TODO: remove the following binding once the TLS Bootstrapping feature is enabled by default, see:
  # https://github.com/kubernetes-incubator/kube-aws/pull/618#discussion_r115162048
  # https://kubernetes.io/docs/admin/authorization/rbac/#core-component-roles

  # Associates the add-on role `kube-aws:node-extensions` to all nodes, so that
  # extra kube-aws features (like node draining) work as expected
  - path: /srv/kubernetes/rbac/cluster-role-bindings/node-extensions.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: kube-aws:node-extensions
      subjects:
        - kind: User
          name: kube-worker
        - kind: Group
          name: system:nodes
      roleRef:
        kind: ClusterRole
        name: kube-aws:node-extensions
        apiGroup: rbac.authorization.k8s.io

  # RBAC Rules to Allow Kubelet authentication to be enabled but accept unauthenticated GET calls to the /metrics endpoint
  - path: /srv/kubernetes/rbac/cluster-roles/node-access.yaml
    content: |
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: node-healthz-access
      rules:
      - apiGroups: [""]
        resources: ["nodes/proxy"]
        verbs: ["create", "get"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: node-logs-metrics
      rules:
      - apiGroups:
        - ""
        resources:
        - nodes/log
        - nodes/stats
        - nodes/metrics
        verbs:
        - "*"

  - path: /srv/kubernetes/rbac/cluster-role-bindings/node-access.yaml
    content: |
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: node-healthz-access
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: node-healthz-access
      subjects:
      - kind: Group
        name: system:unauthenticated
        apiGroup: rbac.authorization.k8s.io
      - kind: Group
        name: system:authenticated
        apiGroup: rbac.authorization.k8s.io
      - kind: User
        name: system:anonymous
        apiGroup: rbac.authorization.k8s.io
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: node-logs-metrics
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: node-logs-metrics
      subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: Group
        name: system:authenticated

  # metrics-server
  - path: /srv/kubernetes/rbac/cluster-role-bindings/metrics-server.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: metrics-server:system:auth-delegator
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:auth-delegator
      subjects:
      - kind: ServiceAccount
        name: metrics-server
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: system:metrics-server
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:metrics-server
      subjects:
      - kind: ServiceAccount
        name: metrics-server
        namespace: kube-system

  - path: /srv/kubernetes/rbac/role-bindings/metrics-server.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: RoleBinding
      metadata:
        name: metrics-server-auth-reader
        namespace: kube-system
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: extension-apiserver-authentication-reader
      subjects:
      - kind: ServiceAccount
        name: metrics-server
        namespace: kube-system

  - path: /srv/kubernetes/rbac/cluster-roles/metrics-server.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: system:metrics-server
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - nodes
        - nodes/stats
        - namespaces
        verbs:
        - get
        - list
        - watch
      - apiGroups:
        - "extensions"
        resources:
        - deployments
        verbs:
        - get
        - list
        - watch
      ---
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: system:aggregated-metrics-reader
        labels:
          rbac.authorization.k8s.io/aggregate-to-view: "true"
          rbac.authorization.k8s.io/aggregate-to-edit: "true"
          rbac.authorization.k8s.io/aggregate-to-admin: "true"
      rules:
      - apiGroups: ["metrics.k8s.io"]
        resources: ["pods"]
        verbs: ["get", "list", "watch"]

  - path: /srv/kubernetes/rbac/cluster-role-bindings/nodes-can-create-csrs.yaml
    content: |
      # enable bootstrapping nodes to create CSR
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: create-csrs-for-bootstrapping
      subjects:
      - kind: Group
        name: system:bootstrappers
        apiGroup: rbac.authorization.k8s.io
      roleRef:
        kind: ClusterRole
        name: system:node-bootstrapper
        apiGroup: rbac.authorization.k8s.io

  - path: /srv/kubernetes/rbac/cluster-role-bindings/automatically-sign-node-certificate-requests.yaml
    content: |
      # Approve all CSRs for the group "system:bootstrappers"
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: auto-approve-csrs-for-group
      subjects:
      - kind: Group
        name: system:bootstrappers
        apiGroup: rbac.authorization.k8s.io
      roleRef:
        kind: ClusterRole
        name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
        apiGroup: rbac.authorization.k8s.io

  - path: /srv/kubernetes/rbac/cluster-role-bindings/automatically-sign-node-certificate-renewals.yaml
    content: |
      # Approve renewal CSRs for the group "system:nodes"
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: auto-approve-renewals-for-nodes
      subjects:
      - kind: Group
        name: system:nodes
        apiGroup: rbac.authorization.k8s.io
      roleRef:
        kind: ClusterRole
        name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
        apiGroup: rbac.authorization.k8s.io

  - path: /srv/kubernetes/manifests/kube-proxy-cm.yaml
    content: |
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: kube-proxy-config
        namespace: kube-system
      data:
        kube-proxy-config.yaml: |
          apiVersion: {{if checkVersion ">=1.9" .K8sVer}}kubeproxy.config.k8s.io{{else}}componentconfig{{end}}/v1alpha1
          kind: KubeProxyConfiguration
          bindAddress: 0.0.0.0
          metricsBindAddress: 0.0.0.0:10249
          {{range $flag,$value := .KubeProxy.Config -}}
          {{$flag}}: {{$value}}
          {{ end -}}
          clientConnection:
            kubeconfig: /etc/kubernetes/kubeconfig/kube-proxy.yaml
          clusterCIDR: {{.PodCIDR}}
          {{if .KubeProxy.IPVSMode.Enabled -}}
          {{if checkVersion ">=1.10" .K8sVer -}}
          featureGates:
            SupportIPVSProxyMode: true
          {{else -}}
          featureGates: "SupportIPVSProxyMode=true"
          {{end -}}
          mode: ipvs
          ipvs:
            scheduler: {{.KubeProxy.IPVSMode.Scheduler}}
            syncPeriod: {{.KubeProxy.IPVSMode.SyncPeriod}}
            minSyncPeriod: {{.KubeProxy.IPVSMode.MinSyncPeriod}}
          {{end}}

  - path: /srv/kubernetes/manifests/kube-proxy-ds.yaml
    content: |
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: kube-proxy
        namespace: kube-system
        labels:
          k8s-app: kube-proxy
      spec:
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        selector:
          matchLabels:
            k8s-app: kube-proxy
        template:
          metadata:
            labels:
              k8s-app: kube-proxy
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            serviceAccountName: kube-proxy
            tolerations:
            - operator: Exists
              effect: NoSchedule
            - operator: Exists
              effect: NoExecute
            - operator: Exists
              key: CriticalAddonsOnly
            hostNetwork: true
            containers:
            - name: kube-proxy
              image: {{.HyperkubeImage.RepoWithTag}}
              command:
              - /hyperkube
              - kube-proxy
              - --config=/etc/kubernetes/kube-proxy/kube-proxy-config.yaml
              securityContext:
                privileged: true
              volumeMounts:
              {{if .KubeProxy.IPVSMode.Enabled -}}
              - mountPath: /lib/modules
                name: lib-modules
                readOnly: true
              {{end -}}
              - mountPath: /etc/kubernetes/kubeconfig
                name: kubeconfig
                readOnly: true
              - mountPath: /etc/kubernetes/kube-proxy
                name: kube-proxy-config
                readOnly: true
            volumes:
            {{if .KubeProxy.IPVSMode.Enabled -}}
            - name: lib-modules
              hostPath:
                path: /lib/modules
            {{end -}}
            - name: kubeconfig
              hostPath:
                path: /etc/kubernetes/kubeconfig
            - name: kube-proxy-config
              configMap:
                name: kube-proxy-config

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        labels:
          k8s-app: kube-apiserver
      spec:
        priorityClassName: system-node-critical
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: {{.HyperkubeImage.RepoWithTag}}
          command:
          - /hyperkube
          - kube-apiserver
          {{- if .ApiServerLeaseEndpointReconciler }}
          - --endpoint-reconciler-type=lease
          {{- else }}
          - --apiserver-count={{if .MinControllerCount}}{{ .MinControllerCount }}{{else}}{{ .Controller.Count }}{{end}}
          {{- end }}
          - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,PersistentVolumeClaimResize,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,RuntimeClass,ResourceQuota,ExtendedResourceToleration,NodeRestriction,PodSecurityPolicy{{if .Experimental.Admission.AlwaysPullImages.Enabled}},AlwaysPullImages{{ end }}{{ if .Experimental.Admission.EventRateLimit.Enabled }},EventRateLimit{{end}}
          {{ if .Experimental.Admission.EventRateLimit.Enabled -}}
          - --admission-control-config-file=/etc/kubernetes/auth/admission-control-config.yaml
          {{ end -}}
          - --bind-address=0.0.0.0
          - --etcd-servers=#ETCD_ENDPOINTS#
          - --etcd-cafile=/etc/kubernetes/ssl/etcd-trusted-ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem
          - --allow-privileged=true
          - --service-cluster-ip-range={{.ServiceCIDR}}
          - --insecure-port=0
          - --secure-port=443
          - --enable-bootstrap-token-auth=true
          - --token-auth-file=/etc/kubernetes/auth/tokens.csv
          - --storage-backend=etcd3
          {{ if .Kubernetes.KubeApiServer.TargetRamMb -}}
          - --target-ram-mb={{.Kubernetes.KubeApiServer.TargetRamMb}}
          {{ end -}}
          - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
          {{if .Experimental.AuditLog.Enabled}}
          - --audit-log-maxage={{.Experimental.AuditLog.MaxAge}}
          - --audit-log-maxsize={{.Experimental.AuditLog.MaxSize}}
          - --audit-log-path={{.Experimental.AuditLog.LogPath}}
          - --audit-log-maxbackup={{.Experimental.AuditLog.MaxBackup}}
          - --audit-policy-file=/etc/kubernetes/apiserver/audit-policy.yaml
          {{ end }}
          - --authorization-mode=Node,RBAC
          {{if .Experimental.Authentication.Webhook.Enabled}}
          - --authentication-token-webhook-config-file=/etc/kubernetes/webhooks/authentication.yaml
          - --authentication-token-webhook-cache-ttl={{ .Experimental.Authentication.Webhook.CacheTTL }}
          {{ end }}
          - --advertise-address=$private_ipv4
          - --anonymous-auth=false
          {{if .Experimental.Oidc.Enabled}}
          - --oidc-issuer-url={{.Experimental.Oidc.IssuerUrl}}
          - --oidc-client-id={{.Experimental.Oidc.ClientId}}
          {{if .Experimental.Oidc.UsernameClaim}}
          - --oidc-username-claim={{.Experimental.Oidc.UsernameClaim}}
          {{ end -}}
          {{if .Experimental.Oidc.GroupsClaim}}
          - --oidc-groups-claim={{.Experimental.Oidc.GroupsClaim}}
          {{ end -}}
          {{ end -}}
          {{if .Kubernetes.EncryptionAtRest.Enabled}}
          - --experimental-encryption-provider-config=/etc/kubernetes/additional-configs/encryption-config.yaml
          {{end}}
          - --cert-dir=/etc/kubernetes/ssl
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/service-account-key.pem
          - --runtime-config=networking.k8s.io/v1/networkpolicies=true,policy/v1beta1/podsecuritypolicy=true
          {{- if .ControllerFeatureGates.Enabled }}
          - --feature-gates={{.ControllerFeatureGates.String}}
          {{- end }}
          {{ if .Addons.APIServerAggregator.Enabled -}}
          - --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --requestheader-allowed-names=aggregator
          - --requestheader-extra-headers-prefix=X-Remote-Extra-
          - --requestheader-group-headers=X-Remote-Group
          - --requestheader-username-headers=X-Remote-User
          - --enable-aggregator-routing=false
          - --proxy-client-cert-file=/etc/kubernetes/ssl/apiserver-aggregator.pem
          - --proxy-client-key-file=/etc/kubernetes/ssl/apiserver-aggregator-key.pem
          {{ end -}}
          {{range $f := .APIServerFlags}}
          - --{{$f.Name}}={{$f.Value}}
          {{ end -}}
          {{ if .Kubernetes.KubeApiServer.ComputeResources -}}
          resources:
            {{ if .Kubernetes.KubeApiServer.ComputeResources.Requests -}}
            requests:
              {{ if .Kubernetes.KubeApiServer.ComputeResources.Requests.Cpu -}}
              cpu: {{.Kubernetes.KubeApiServer.ComputeResources.Requests.Cpu }}
              {{ end -}}
              {{ if .Kubernetes.KubeApiServer.ComputeResources.Requests.Memory -}}
              memory: {{.Kubernetes.KubeApiServer.ComputeResources.Requests.Memory }}
              {{ end -}}
            {{ end }}
            {{ if .Kubernetes.KubeApiServer.ComputeResources.Limits -}}
            limits:
              {{ if .Kubernetes.KubeApiServer.ComputeResources.Limits.Cpu -}}
              cpu: {{.Kubernetes.KubeApiServer.ComputeResources.Limits.Cpu }}
              {{ end -}}
              {{- if .Kubernetes.KubeApiServer.ComputeResources.Limits.Memory -}}
              memory: {{.Kubernetes.KubeApiServer.ComputeResources.Limits.Memory }}
              {{ end -}}
            {{ end }}
          {{ end }}
          livenessProbe:
            tcpSocket:
              port: 443
            initialDelaySeconds: 15
            timeoutSeconds: 15
          readinessProbe:
            tcpSocket:
              port: 443
            initialDelaySeconds: 5
            periodSeconds: 10
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          {{if .Kubernetes.EncryptionAtRest.Enabled}}
          - mountPath: /etc/kubernetes/additional-configs
            name: auth-additional-configs
            readOnly: true
          {{end}}
          - mountPath: /etc/kubernetes/auth
            name: auth-kubernetes
            readOnly: true
          {{if .Experimental.Authentication.Webhook.Enabled}}
          - mountPath: /etc/kubernetes/webhooks
            name: kubernetes-webhooks
            readOnly: true
          {{end}}
          {{if .Experimental.AuditLog.Enabled}}
          - mountPath: /var/log
            name: var-log
            readOnly: false
          - mountPath: /etc/kubernetes/apiserver
            name: apiserver
            readOnly: true
          {{end}}
          {{range $v := .APIServerVolumes}}
          - mountPath: {{quote $v.Path}}
            name: {{quote $v.Name}}
            readOnly: {{$v.ReadOnly}}
          {{end}}
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        {{if .Kubernetes.EncryptionAtRest.Enabled}}
        - hostPath:
            path: /etc/kubernetes/additional-configs
          name: auth-additional-configs
        {{end}}
        - hostPath:
            path: /etc/kubernetes/auth
          name: auth-kubernetes
        {{if .Experimental.Authentication.Webhook.Enabled}}
        - hostPath:
            path: /etc/kubernetes/webhooks
          name: kubernetes-webhooks
        {{end}}
        {{if .Experimental.AuditLog.Enabled}}
        - hostPath:
            path: /var/log
          name: var-log
        - hostPath:
            path: /etc/kubernetes/apiserver
          name: apiserver
        {{end}}
        {{range $v := .APIServerVolumes}}
        - hostPath:
            path: {{quote $v.Path}}
          name: {{quote $v.Name}}
        {{end}}

  {{ if .Experimental.Admission.EventRateLimit.Enabled -}}
  - path: /etc/kubernetes/auth/admission-control-config.yaml
    content: |
      kind: AdmissionConfiguration
      apiVersion: apiserver.k8s.io/v1alpha1
      plugins:
      - name: EventRateLimit
        path: /etc/kubernetes/auth/admission-control-eventconfig.yaml

  - path: /etc/kubernetes/auth/admission-control-eventconfig.yaml
    content: |
      kind: Configuration
      apiVersion: eventratelimit.admission.k8s.io/v1alpha1
      limits:
{{ .Experimental.Admission.EventRateLimit.Limits | indent 6 }}
  {{- end }}

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        labels:
          k8s-app: kube-controller-manager
      spec:
        priorityClassName: system-node-critical
        containers:
        - name: kube-controller-manager
          image: {{.HyperkubeImage.RepoWithTag}}
          command:
          - /hyperkube
          - kube-controller-manager
          {{/* mandatory flags below */}}
          - --cluster-name={{.ClusterName}}
          {{- if not .Experimental.CloudControllerManager.Enabled }}
          - --cloud-provider=aws
          {{- end }}
          - --kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml
          - --authentication-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml
          - --authorization-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml
          - --leader-elect=true
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem
          - --use-service-account-credentials
          {{/* optional flags below */}}
          - --cluster-signing-cert-file=/etc/kubernetes/ssl/worker-ca.pem
          - --cluster-signing-key-file=/etc/kubernetes/ssl/worker-ca-key.pem
          {{ if .Experimental.NodeMonitorGracePeriod }}
          - --node-monitor-grace-period={{ .Experimental.NodeMonitorGracePeriod }}
          {{end}}
          {{ if not .Kubernetes.Networking.AmazonVPC.Enabled -}}
          - --allocate-node-cidrs=true
          - --cluster-cidr={{.PodCIDR}}
          - --configure-cloud-routes=false
          {{ end -}}
          - --service-cluster-ip-range={{.ServiceCIDR}} {{/* removes the service CIDR range from the cluster CIDR if it intersects */}}
          {{ if and (not .Addons.MetricsServer.Enabled) (not .Kubernetes.PodAutoscalerUseRestClient.Enabled) -}}
          - --horizontal-pod-autoscaler-use-rest-clients=false
          {{end}}
          {{range $f := .ControllerFlags -}}
          - --{{$f.Name}}={{$f.Value}}
          {{ end -}}
          {{ if .ControllerFeatureGates.Enabled -}}
          - --feature-gates={{.ControllerFeatureGates.String}}
          {{ end -}}
          resources:
            requests:
              cpu: {{ if .Kubernetes.ControllerManager.ComputeResources.Requests.Cpu }}{{ .Kubernetes.ControllerManager.ComputeResources.Requests.Cpu }}{{ else }}100m{{ end }}
              memory: {{ if .Kubernetes.ControllerManager.ComputeResources.Requests.Memory }}{{ .Kubernetes.ControllerManager.ComputeResources.Requests.Memory }}{{ else }}100M{{ end }}
            limits:
              cpu: {{ if .Kubernetes.ControllerManager.ComputeResources.Limits.Cpu }}{{ .Kubernetes.ControllerManager.ComputeResources.Limits.Cpu }}{{ else }}250m{{ end }}
              memory: {{ if .Kubernetes.ControllerManager.ComputeResources.Limits.Memory }}{{ .Kubernetes.ControllerManager.ComputeResources.Limits.Memory }}{{ else }}512M{{ end }}
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/kubernetes/kubeconfig
            name: kubeconfig
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - name: kubeconfig
          hostPath:
            path: /etc/kubernetes/kubeconfig
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
        labels:
          k8s-app: kube-scheduler
      spec:
        priorityClassName: system-node-critical
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: {{.HyperkubeImage.RepoWithTag}}
          command:
          - /hyperkube
          - kube-scheduler
          - --kubeconfig=/etc/kubernetes/kubeconfig/kube-scheduler.yaml
          - --authentication-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml
          - --authorization-kubeconfig=/etc/kubernetes/kubeconfig/kube-controller-manager.yaml
          - --leader-elect=true
          {{- range $f := .KubeSchedulerFlags }}
          - --{{$f.Name}}={{$f.Value}}
          {{- end }}
          {{- if .ControllerFeatureGates.Enabled }}
          - --feature-gates={{.ControllerFeatureGates.String}}
          {{- end }}
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/kubernetes/kubeconfig
            name: kubeconfig
            readOnly: true
        volumes:
        - name: ssl-certs-kubernetes
          hostPath:
            path: /etc/kubernetes/ssl
        - name: kubeconfig
          hostPath:
            path: /etc/kubernetes/kubeconfig

  {{- if .Addons.Rescheduler.Enabled }}
  - path: /srv/kubernetes/manifests/kube-rescheduler-de.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kube-rescheduler
        namespace: kube-system
        labels:
          k8s-app: kube-rescheduler
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "Rescheduler"
      spec:
        # `replicas` should always be the default of 1, rescheduler crashes otherwise
        selector:
          matchLabels:
            k8s-app: kube-rescheduler
        template:
          metadata:
            labels:
              k8s-app: kube-rescheduler
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            hostNetwork: true
            containers:
            - name: kube-rescheduler
              image: {{ .KubeReschedulerImage.RepoWithTag }}
              resources:
                requests:
                  cpu: 10m
                  memory: 100Mi
  {{- end }}

  {{- if .Experimental.CloudControllerManager.Enabled }}
  - path: /srv/kubernetes/manifests/cloud-controller-manager.yaml
    content: |
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: cloud-controller-manager
      rules:
      - apiGroups:
        - ""
        resources:
        - events
        verbs:
        - create
        - patch
        - update
      - apiGroups:
        - ""
        resources:
        - nodes
        verbs:
        - '*'
      - apiGroups:
        - ""
        resources:
        - nodes/status
        verbs:
        - patch
      - apiGroups:
        - ""
        resources:
        - services
        - secrets
        verbs:
        - list
        - patch
        - update
        - watch
      - apiGroups:
        - ""
        resources:
        - serviceaccounts
        verbs:
        - create
        - list
        - get
      - apiGroups:
        - ""
        resources:
        - persistentvolumes
        verbs:
        - get
        - list
        - update
        - watch
      - apiGroups:
        - ""
        resources:
        - endpoints
        verbs:
        - create
        - get
        - list
        - watch
        - update
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: cloud-controller-manager
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: system:cloud-controller-manager
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cloud-controller-manager
      subjects:
      - kind: ServiceAccount
        name: cloud-controller-manager
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: system:cloud-controller-manager-authentication
        namespace: kube-system
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: extension-apiserver-authentication-reader
      subjects:
      - kind: ServiceAccount
        name: cloud-controller-manager
        namespace: kube-system
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        labels:
          k8s-app: cloud-controller-manager
        name: cloud-controller-manager
        namespace: kube-system
      spec:
        selector:
          matchLabels:
            k8s-app: cloud-controller-manager
        template:
          metadata:
            labels:
              k8s-app: cloud-controller-manager
          spec:
            hostNetwork: true
            serviceAccountName: cloud-controller-manager
            priorityClassName: system-cluster-critical
            containers:
            - name: cloud-controller-manager
              image: {{.HyperkubeImage.RepoWithTag}}
              command:
              - /cloud-controller-manager
              - --cloud-provider=aws
              - --leader-elect=true
              - --use-service-account-credentials
              - --v=4
              # these flags will vary for every cloud provider
              - --cluster-name={{.ClusterName}}
              - --allocate-node-cidrs=false
              - --configure-cloud-routes=false
              {{- if .Experimental.DisableSecurityGroupIngress }}
              - --cloud-config=/etc/kubernetes/cloud-controller-manager/cloud.config
              {{- end }}
              {{- if .Experimental.DisableSecurityGroupIngress }}
              volumeMounts:
              - mountPath: /etc/kubernetes
                name: etc-kubernetes
                readOnly: true
              {{- end }}
            tolerations:
            # this is required so CCM can bootstrap itself
            - key: node.cloudprovider.kubernetes.io/uninitialized
              value: "true"
              effect: NoSchedule
            # this is to have the daemonset runnable on master nodes
            # the taint may vary depending on your cluster setup
            - key: "node.kubernetes.io/role"
              operator: "Equal"
              value: "master"
              effect: "NoSchedule"
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            # this is to restrict CCM to only run on master nodes
            # the node selector may vary depending on your cluster setup
            nodeSelector:
              node.kubernetes.io/role: "master"
            {{ if .Experimental.DisableSecurityGroupIngress -}}
            volumes:
            - hostPath:
                path: /etc/kubernetes
              name: etc-kubernetes
            {{- end }}
  {{- end }}

  {{- if .Experimental.ContainerStorageInterface.Enabled }}
  - path: /srv/kubernetes/manifests/csi-controller.yaml
    content: |
      ---
      # Controller Service
      kind: Deployment
      apiVersion: apps/v1
      metadata:
        name: ebs-csi-controller
        namespace: kube-system
      spec:
        replicas: 2
        selector:
          matchLabels:
            app: ebs-csi-controller
        template:
          metadata:
            labels:
              app: ebs-csi-controller
          spec:
            nodeSelector:
              beta.kubernetes.io/os: linux
              node.kubernetes.io/role: master
            serviceAccount: ebs-csi-controller-sa
            priorityClassName: system-cluster-critical
            tolerations:
              - key: "node.kubernetes.io/role"
                operator: "Equal"
                value: "master"
                effect: "NoSchedule"
              - key: "CriticalAddonsOnly"
                operator: "Exists"
            containers:
              - name: ebs-plugin
                image: {{ .Experimental.ContainerStorageInterface.AmazonEBSDriver.RepoWithTag }}
                args :
                # - {all,controller,node} # specify the driver mode
                  - --endpoint=$(CSI_ENDPOINT)
                  - --logtostderr
                  {{- if .Experimental.ContainerStorageInterface.Debug }}
                  - --v=9
                  {{- end }}
                env:
                  - name: CSI_ENDPOINT
                    value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: aws-secret
                        key: key_id
                        optional: true
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: aws-secret
                        key: access_key
                        optional: true
                # overwrite the AWS region instead of looking it up dynamically via the AWS EC2 metadata svc
                # - name: AWS_REGION
                #   value: us-east-1
                volumeMounts:
                  - name: socket-dir
                    mountPath: /var/lib/csi/sockets/pluginproxy/
                ports:
                  - name: healthz
                    containerPort: 9808
                    protocol: TCP
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: healthz
                  initialDelaySeconds: 10
                  timeoutSeconds: 3
                  periodSeconds: 10
                  failureThreshold: 5
              - name: csi-provisioner
                image: {{ .Experimental.ContainerStorageInterface.CSIProvisioner.RepoWithTag }}
                args:
                  - --csi-address=$(ADDRESS)
                  {{- if .Experimental.ContainerStorageInterface.Debug }}
                  - --v=9
                  {{- end }}
                  - --feature-gates=Topology=true
                  - --enable-leader-election
                  - --leader-election-type=leases
                env:
                  - name: ADDRESS
                    value: /var/lib/csi/sockets/pluginproxy/csi.sock
                volumeMounts:
                  - name: socket-dir
                    mountPath: /var/lib/csi/sockets/pluginproxy/
              - name: csi-attacher
                image: {{ .Experimental.ContainerStorageInterface.CSIAttacher.RepoWithTag }}
                args:
                  - --csi-address=$(ADDRESS)
                  {{- if .Experimental.ContainerStorageInterface.Debug }}
                  - --v=9
                  {{- end }}
                env:
                  - name: ADDRESS
                    value: /var/lib/csi/sockets/pluginproxy/csi.sock
                volumeMounts:
                  - name: socket-dir
                    mountPath: /var/lib/csi/sockets/pluginproxy/
              - name: liveness-probe
                image: {{ .Experimental.ContainerStorageInterface.CSILivenessProbe.RepoWithTag }}
                args:
                  - --csi-address=/csi/csi.sock
                volumeMounts:
                  - name: socket-dir
                    mountPath: /csi
            volumes:
              - name: socket-dir
                emptyDir: {}
  - path: /srv/kubernetes/manifests/csi-node.yaml
    content: |
      ---
      # Node Service
      kind: DaemonSet
      apiVersion: apps/v1
      metadata:
        name: ebs-csi-node
        namespace: kube-system
      spec:
        selector:
          matchLabels:
            app: ebs-csi-node
        template:
          metadata:
            labels:
              app: ebs-csi-node
          spec:
            nodeSelector:
              beta.kubernetes.io/os: linux
            hostNetwork: true
            priorityClassName: system-node-critical
            tolerations:
              - operator: Exists
            containers:
              - name: ebs-plugin
                securityContext:
                  privileged: true
                image: {{ .Experimental.ContainerStorageInterface.AmazonEBSDriver.RepoWithTag }}
                args:
                  - --endpoint=$(CSI_ENDPOINT)
                  - --logtostderr
                  {{- if .Experimental.ContainerStorageInterface.Debug }}
                  - --v=9
                  {{- end }}
                env:
                  - name: CSI_ENDPOINT
                    value: unix:/csi/csi.sock
                volumeMounts:
                  - name: kubelet-dir
                    mountPath: /var/lib/kubelet
                    mountPropagation: "Bidirectional"
                  - name: plugin-dir
                    mountPath: /csi
                  - name: device-dir
                    mountPath: /dev
                ports:
                  - name: healthz
                    containerPort: 9808
                    protocol: TCP
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: healthz
                  initialDelaySeconds: 10
                  timeoutSeconds: 3
                  periodSeconds: 10
                  failureThreshold: 5
              - name: node-driver-registrar
                image: {{ .Experimental.ContainerStorageInterface.CSINodeDriverRegistrar.RepoWithTag }}
                args:
                  - --csi-address=$(ADDRESS)
                  - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
                  {{- if .Experimental.ContainerStorageInterface.Debug }}
                  - --v=5
                  {{- end }}
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "rm -rf /registration/ebs.csi.aws.com-reg.sock /csi/csi.sock"]
                env:
                  - name: ADDRESS
                    value: /csi/csi.sock
                  - name: DRIVER_REG_SOCK_PATH
                    value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock
                volumeMounts:
                  - name: plugin-dir
                    mountPath: /csi
                  - name: registration-dir
                    mountPath: /registration
              - name: liveness-probe
                image: {{ .Experimental.ContainerStorageInterface.CSILivenessProbe.RepoWithTag }}
                args:
                  - --csi-address=/csi/csi.sock
                volumeMounts:
                  - name: plugin-dir
                    mountPath: /csi
            volumes:
              - name: kubelet-dir
                hostPath:
                  path: /var/lib/kubelet
                  type: Directory
              - name: plugin-dir
                hostPath:
                  path: /var/lib/kubelet/plugins/ebs.csi.aws.com/
                  type: DirectoryOrCreate
              - name: registration-dir
                hostPath:
                  path: /var/lib/kubelet/plugins_registry/
                  type: Directory
              - name: device-dir
                hostPath:
                  path: /dev
                  type: Directory
  - path: /srv/kubernetes/manifests/csi-rbac.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: ebs-csi-controller-sa
        namespace: kube-system
        #Enable if EKS IAM for SA is used
        #annotations:
        #  eks.amazonaws.com/role-arn: arn:aws:iam::586565787010:role/ebs-csi-role
      ---
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: ebs-external-provisioner-role
      rules:
        - apiGroups: [""]
          resources: ["persistentvolumes"]
          verbs: ["get", "list", "watch", "create", "delete"]
        - apiGroups: [""]
          resources: ["persistentvolumeclaims"]
          verbs: ["get", "list", "watch", "update"]
        - apiGroups: ["storage.k8s.io"]
          resources: ["storageclasses"]
          verbs: ["get", "list", "watch"]
        - apiGroups: [""]
          resources: ["events"]
          verbs: ["list", "watch", "create", "update", "patch"]
        - apiGroups: ["snapshot.storage.k8s.io"]
          resources: ["volumesnapshots"]
          verbs: ["get", "list"]
        - apiGroups: ["snapshot.storage.k8s.io"]
          resources: ["volumesnapshotcontents"]
          verbs: ["get", "list"]
        - apiGroups: ["storage.k8s.io"]
          resources: ["csinodes"]
          verbs: ["get", "list", "watch"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["coordination.k8s.io"]
          resources: ["leases"]
          verbs: ["get", "watch", "list", "delete", "update", "create"]
      ---
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: ebs-csi-provisioner-binding
      subjects:
        - kind: ServiceAccount
          name: ebs-csi-controller-sa
          namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: ebs-external-provisioner-role
        apiGroup: rbac.authorization.k8s.io
      ---
      kind: ClusterRole
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: ebs-external-attacher-role
      rules:
        - apiGroups: [""]
          resources: ["persistentvolumes"]
          verbs: ["get", "list", "watch", "update"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["csi.storage.k8s.io"]
          resources: ["csinodeinfos"]
          verbs: ["get", "list", "watch"]
        - apiGroups: ["storage.k8s.io"]
          resources: ["volumeattachments"]
          verbs: ["get", "list", "watch", "update"]
      ---
      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1
      metadata:
        name: ebs-csi-attacher-binding
      subjects:
        - kind: ServiceAccount
          name: ebs-csi-controller-sa
          namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: ebs-external-attacher-role
        apiGroup: rbac.authorization.k8s.io
      # Fix for nodes being able to access volumeattachment objects
      # see https://github.com/rancher/k3s/issues/732
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: system:nodes:volumeattachments
      rules:
      - apiGroups:
        - storage.k8s.io
        resources:
        - volumeattachments
        verbs:
        - list
        - get
        - create
        - watch
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: system:nodes:volumeattachments
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:nodes:volumeattachments
      subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: Group
        name: system:nodes
  - path: /srv/kubernetes/manifests/csi-driver.yaml
    content: |
      ---
      apiVersion: storage.k8s.io/v1beta1
      kind: CSIDriver
      metadata:
        name: ebs.csi.aws.com
      spec:
        attachRequired: true
        podInfoOnMount: false
  {{- end }}

  - path: /srv/kubernetes/manifests/kube-proxy-sa.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-proxy
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"

{{- if eq .KubeDns.Provider "coredns" }}
  - path: /srv/kubernetes/manifests/coredns-sa.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: coredns
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"

  - path: /srv/kubernetes/manifests/coredns-cr.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        labels:
          kubernetes.io/bootstrapping: rbac-defaults
          addonmanager.kubernetes.io/mode: EnsureExists
        name: system:coredns
      rules:
      - apiGroups:
        - ""
        resources:
        - endpoints
        - services
        - pods
        - namespaces
        verbs:
        - list
        - watch
      - apiGroups:
        - ""
        resources:
        - nodes
        verbs:
        - get

  - path: /srv/kubernetes/manifests/coredns-crb.yaml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        annotations:
          rbac.authorization.kubernetes.io/autoupdate: "true"
        labels:
          kubernetes.io/bootstrapping: rbac-defaults
          addonmanager.kubernetes.io/mode: EnsureExists
        name: system:coredns
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: system:coredns
      subjects:
      - kind: ServiceAccount
        name: coredns
        namespace: kube-system

  - path: /srv/kubernetes/manifests/coredns-cm.yaml
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: coredns
        namespace: kube-system
      data:
        Corefile: |
          {{- if and (eq .KubeDns.Provider "coredns") .KubeDns.AdditionalZoneCoreDNSConfig }}
{{ .KubeDns.AdditionalZoneCoreDNSConfig | indent 10 }}
          {{- end }}
          .:53 {
              errors
              health
              kubernetes cluster.local in-addr.arpa ip6.arpa {
                  pods insecure
                  upstream
                  fallthrough in-addr.arpa ip6.arpa
                  ttl {{ .KubeDns.TTL }}
              }
              forward . /etc/resolv.conf {
                  except cluster.local
                  health_check 5s
              }
              {{- if and (eq .KubeDns.Provider "coredns") .KubeDns.ExtraCoreDNSConfig }}
{{ .KubeDns.ExtraCoreDNSConfig | indent 16 }}
              {{- end }}
              prometheus :9153
              cache {{ .KubeDns.TTL }}
              loop
              reload
              loadbalance
          }
{{- else }}
  - path: /srv/kubernetes/manifests/kube-dns-sa.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"

  - path: /srv/kubernetes/manifests/kube-dns-cm.yaml
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-dns
        namespace: kube-system
{{- end }}

  - path: /srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kube-dns-autoscaler
        namespace: kube-system
        labels:
          k8s-app: kube-dns-autoscaler
          kubernetes.io/cluster-service: "true"
      spec:
        selector:
          matchLabels:
            k8s-app: kube-dns-autoscaler
        template:
          metadata:
            labels:
              k8s-app: kube-dns-autoscaler
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-cluster-critical
            tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            containers:
            - name: autoscaler
              image: {{ .ClusterProportionalAutoscalerImage.RepoWithTag }}
              resources:
                  requests:
                      cpu: "20m"
                      memory: "50Mi
                  limits:
                      memory: "50Mi"
              command:
                - /cluster-proportional-autoscaler
                - --namespace=kube-system
                - --configmap=kube-dns-autoscaler
                {{- if eq .KubeDns.Provider "coredns" }}
                - --target=Deployment/coredns
                {{- else }}
                - --target=Deployment/kube-dns
                {{- end }}
                - --default-params={"linear":{"coresPerReplica":{{ .KubeDns.Autoscaler.CoresPerReplica }},"nodesPerReplica":{{ .KubeDns.Autoscaler.NodesPerReplica }},"min":{{ .KubeDns.Autoscaler.Min}}}}
                - --v=2
                - --logtostderr

  - path: /srv/kubernetes/manifests/dnsmasq-node-coredns-local.yaml
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: coredns-local
        namespace: kube-system
        labels:
          application: coredns
      data:
        Corefile: |
          {{- if and (eq .KubeDns.Provider "coredns") .KubeDns.AdditionalZoneCoreDNSConfig }}
{{ .KubeDns.AdditionalZoneCoreDNSConfig | indent 12 }}
          {{- end }}

          cluster.local:9254 {{ .PodCIDR }}:9254 {{ .ServiceCIDR }}:9254 {
              errors
              kubernetes {
                  pods insecure
              }
              cache 30
              log svc.svc.cluster.local.
              prometheus :9153
          }

          .:9254 {
              errors
              health :9154 # this is global for all servers
              prometheus :9153
              forward . /etc/resolv.conf
              pprof 127.0.0.1:9156
              cache 30
              reload
          }

{{ if .KubeDns.NodeLocalResolver }}
  - path: /srv/kubernetes/manifests/dnsmasq-node-ds.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: dnsmasq
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: dnsmasq
      rules:
        - apiGroups: [""]
          resources: ["endpoints", "services", "pods", "namespaces"]
          verbs: ["list", "watch"]
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: dnsmasq
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: dnsmasq
      subjects:
        - kind: ServiceAccount
          name: dnsmasq
          namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: dnsmasq-privileged-psp
        namespace: kube-system
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: privileged-psp
      subjects:
        - kind: ServiceAccount
          name: dnsmasq
          namespace: kube-system
      ---
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: dnsmasq-node
        namespace: kube-system
        labels:
          k8s-app: dnsmasq-node
      spec:
        selector:
          matchLabels:
            k8s-app: dnsmasq-node
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 10%
          type: RollingUpdate
        selector:
          matchLabels:
            k8s-app: dnsmasq-node
        template:
          metadata:
            labels:
              k8s-app: dnsmasq-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            tolerations:
            - operator: Exists
              effect: NoSchedule
            - operator: Exists
              effect: NoExecute
            volumes:
            - name: kube-dns-config
              configMap:
                name: kube-dns
                optional: true
            {{ if .KubeDns.DNSMasq.CoreDNSLocal.Enabled }}
            - name: coredns-local-config
              configMap:
                name: coredns-local
                items:
                  - key: Corefile
                    path: Corefile
            {{ end }}
            containers:
            - name: dnsmasq
              image: {{ .KubeDnsMasqImage.RepoWithTag }}
              livenessProbe:
                httpGet:
                  path: /healthcheck/dnsmasq
                  port: 9054
                  scheme: HTTP
                initialDelaySeconds: 60
                periodSeconds: 10
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - -v=2
              - -logtostderr
              - -configDir=/etc/k8s/dns/dnsmasq-nanny
              - -restartDnsmasq=true
              - --
              - -k
              - --cache-size={{ .KubeDns.DNSMasq.CacheSize }}
              - --dns-forward-max={{ .KubeDns.DNSMasq.DNSForwardMax }}
              - --log-facility=-
              {{ if .KubeDns.DNSMasq.CoreDNSLocal.Enabled }}
              - --no-resolv
              - --keep-in-foreground
              - --neg-ttl={{ .KubeDns.DNSMasq.NegTTL }}
              - --strict-order
              - --server=127.0.0.1#9254
              {{ else }}
              - --server=//{{.DNSServiceIP}}
              - --server=/cluster.local/{{.DNSServiceIP}}
              - --server=/in-addr.arpa/{{.DNSServiceIP}}
              - --server=/ip6.arpa/{{.DNSServiceIP}}
              {{ end }}
              {{- if ne (len .KubeDns.NodeLocalResolverOptions) 0 }}
                {{- range .KubeDns.NodeLocalResolverOptions }}
              - {{.}}
                {{- end }}
              {{- end }}
              ports:
              - containerPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                name: dns-tcp
                protocol: TCP
              # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
              resources:
                limits:
                  cpu: 100m
                  memory: 50Mi
                requests:
                  ephemeral-storage: 256Mi
              volumeMounts:
              - name: kube-dns-config
                mountPath: /etc/k8s/dns/dnsmasq-nanny
            - name: sidecar
              image: {{ .DnsMasqMetricsImage.RepoWithTag }}
              livenessProbe:
                httpGet:
                  path: /metrics
                  port: 9054
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - --v=2
              - --logtostderr
              {{ if .KubeDns.DNSMasq.CoreDNSLocal.Enabled }}
              - --probe=dnsmasq,127.0.0.1:9254,ec2.amazonaws.com,5,A
              {{ else }}
              - --probe=dnsmasq,127.0.0.1:53,ec2.amazonaws.com,5,A
              {{ end }}
              ports:
              - containerPort: 9054
                name: metrics
                protocol: TCP
              resources:
                requests:
                  ephemeral-storage: 256Mi
                limits:
                  cpu: 10m
                  memory: 45Mi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
            {{ if .KubeDns.DNSMasq.CoreDNSLocal.Enabled }}
            - name: coredns
              image: {{ .CoreDnsImage.RepoWithTag }}
              args: ["-conf", "/etc/coredns/Corefile"]
              volumeMounts:
                - name: coredns-local-config
                  mountPath: /etc/coredns
              ports:
                - containerPort: 9254
                  name: dns
                  protocol: UDP
                - containerPort: 9254
                  name: dns-tcp
                  protocol: TCP
              livenessProbe:
                httpGet:
                  path: /health
                  port: 9154
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              resources:
                requests:
                  ephemeral-storage: 256Mi
                  {{ if .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Requests.Cpu }}
                  cpu: {{ .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Requests.Cpu }}
                  {{ end }}
                  {{ if .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Requests.Memory }}
                  memory: {{ .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Requests.Memory }}
                  {{ end }}
                {{ if or .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Cpu .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Memory }}
                limits:
                  {{ if .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Cpu }}
                  cpu: {{ .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Cpu }}
                  {{ end }}
                  {{ if .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Memory }}
                  memory: {{ .KubeDns.DNSMasq.CoreDNSLocal.ComputeResources.Limits.Memory }}
                  {{ end }}
                {{ end }}
            {{ end }}
            hostNetwork: true
            dnsPolicy: Default
            automountServiceAccountToken: true
            serviceAccountName: dnsmasq
{{ end }}

{{- if eq .KubeDns.Provider "coredns" }}
  - path: /srv/kubernetes/manifests/coredns-de.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: coredns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
          addonmanager.kubernetes.io/mode: Reconcile
          kubernetes.io/name: "CoreDNS"
      spec:
        # replicas: not specified here:
        # 1. In order to make Addon Manager do not reconcile this replicas parameter.
        # 2. Default is 1.
        # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
        strategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 1
        selector:
          matchLabels:
            k8s-app: kube-dns
        template:
          metadata:
            labels:
              k8s-app: kube-dns
            annotations:
              prometheus.io/port: "9153"
              prometheus.io/scrape: "true"
              seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
          spec:
            priorityClassName: system-node-critical
            serviceAccountName: coredns
            {{ if or .KubeDns.DeployToControllers .KubeDns.AntiAffinityAvailabilityZone -}}
            affinity:
              {{- if .KubeDns.DeployToControllers }}
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: node.kubernetes.io/role
                      operator: In
                      values:
                      - "master"
              {{- end }}
              {{- if .KubeDns.AntiAffinityAvailabilityZone }}
              podAntiAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                - podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                      - key: k8s-app
                        operator: In
                        values:
                        - "kube-dns"
                    topologyKey: "failure-domain.beta.kubernetes.io/zone"
                  weight: 1
              {{- end }}
            tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              - key: "node.kubernetes.io/role"
                operator: "Equal"
                value: "master"
                effect: "NoSchedule"
            {{ else -}}
            tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            {{ end -}}
            containers:
            - name: coredns
              image: {{ .CoreDnsImage.RepoWithTag }}
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: {{ .KubeDns.DnsDeploymentResources.Limits.Cpu }}
                  memory: {{ .KubeDns.DnsDeploymentResources.Limits.Memory }}
                requests:
                  cpu: {{ .KubeDns.DnsDeploymentResources.Requests.Cpu }}
                  memory: {{ .KubeDns.DnsDeploymentResources.Requests.Memory }}
              args: [ "-conf", "/etc/coredns/Corefile" ]
              volumeMounts:
              - name: config-volume
                mountPath: /etc/coredns
              ports:
              - containerPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                name: dns-tcp
                protocol: TCP
              - containerPort: 9153
                name: metrics
                protocol: TCP
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
            dnsPolicy: Default
            volumes:
              - name: config-volume
                configMap:
                  name: coredns
                  items:
                  - key: Corefile
                    path: Corefile
{{- else }}
  - path: /srv/kubernetes/manifests/kube-dns-de.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
      spec:
        # replicas: not specified here:
        # 1. In order to make Addon Manager do not reconcile this replicas parameter.
        # 2. Default is 1.
        # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
        strategy:
          rollingUpdate:
            maxSurge: 10%
            maxUnavailable: 0
        selector:
          matchLabels:
            k8s-app: kube-dns
        template:
          metadata:
            labels:
              k8s-app: kube-dns
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-cluster-critical
            volumes:
            - name: kube-dns-config
              configMap:
                name: kube-dns
                optional: true
            {{ if .KubeDns.DeployToControllers -}}
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: node.kubernetes.io/role
                      operator: In
                      values:
                      - "master"
            tolerations:
              - key: "CriticalAddonsOnly"
                operator: "Exists"
              - key: "node.kubernetes.io/role"
                operator: "Equal"
                value: "master"
                effect: "NoSchedule"
            {{ else -}}
            tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            {{ end -}}
            containers:
            - name: kubedns
              image: {{ .KubeDnsImage.RepoWithTag }}
              resources:
                limits:
                  cpu: {{ .KubeDns.DnsDeploymentResources.Limits.Cpu }}
                  memory: {{ .KubeDns.DnsDeploymentResources.Limits.Memory }}
                requests:
                  cpu: {{ .KubeDns.DnsDeploymentResources.Requests.Cpu }}
                  memory: {{ .KubeDns.DnsDeploymentResources.Requests.Memory }}
              livenessProbe:
                httpGet:
                  path: /healthcheck/kubedns
                  port: 10054
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 8081
                  scheme: HTTP
                initialDelaySeconds: 3
                timeoutSeconds: 5
              args:
              - --domain=cluster.local.
              - --dns-port=10053
              - --config-dir=/kube-dns-config
              # This should be set to v=2 only after the new image (cut from 1.5) has
              # been released, otherwise we will flood the logs.
              - --v=2
              env:
              - name: PROMETHEUS_PORT
                value: "10055"
              ports:
              - containerPort: 10053
                name: dns-local
                protocol: UDP
              - containerPort: 10053
                name: dns-tcp-local
                protocol: TCP
              - containerPort: 10055
                name: metrics
                protocol: TCP
              volumeMounts:
              - name: kube-dns-config
                mountPath: /kube-dns-config
            - name: dnsmasq
              image: {{ .KubeDnsMasqImage.RepoWithTag }}
              livenessProbe:
                httpGet:
                  path: /healthcheck/dnsmasq
                  port: 10054
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - -v=2
              - -logtostderr
              - -configDir=/etc/k8s/dns/dnsmasq-nanny
              - -restartDnsmasq=true
              - --
              - -k
              - --cache-size=1000
              - --log-facility=-
              - --server=/cluster.local/127.0.0.1#10053
              - --server=/in-addr.arpa/127.0.0.1#10053
              - --server=/ip6.arpa/127.0.0.1#10053
              ports:
              - containerPort: 53
                name: dns
                protocol: UDP
              - containerPort: 53
                name: dns-tcp
                protocol: TCP
              # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
              resources:
                requests:
                  cpu: 150m
                  memory: 20Mi
              volumeMounts:
              - name: kube-dns-config
                mountPath: /etc/k8s/dns/dnsmasq-nanny
            - name: sidecar
              image: {{ .DnsMasqMetricsImage.RepoWithTag }}
              livenessProbe:
                httpGet:
                  path: /metrics
                  port: 10054
                  scheme: HTTP
                initialDelaySeconds: 60
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 5
              args:
              - --v=2
              - --logtostderr
              - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
              - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
              ports:
              - containerPort: 10054
                name: metrics
                protocol: TCP
              resources:
                requests:
                  memory: 20Mi
                  cpu: 10m
            dnsPolicy: Default
            serviceAccountName: kube-dns
{{- end }}

  - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
    content: |
      apiVersion: v1
      kind: Service
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
          addonmanager.kubernetes.io/mode: Reconcile
          {{- if eq .KubeDns.Provider "coredns" }}
          kubernetes.io/name: "CoreDNS"
          {{- else }}
          kubernetes.io/name: "KubeDNS"
          {{- end }}
      spec:
        selector:
          k8s-app: kube-dns
        clusterIP: {{.DNSServiceIP}}
        ports:
        - name: dns
          port: 53
          protocol: UDP
        - name: dns-tcp
          port: 53
          protocol: TCP

  - path: /srv/kubernetes/manifests/kube-dns-pdb.yaml
    content: |
      apiVersion: policy/v1beta1
      kind: PodDisruptionBudget
      metadata:
        name: kube-dns
        namespace: kube-system
      spec:
        maxUnavailable: 1
        selector:
          matchLabels:
            k8s-app: kube-dns

  - path: /srv/kubernetes/manifests/metrics-server-sa.yaml
    content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: metrics-server
          namespace: kube-system
          labels:

  - path: /srv/kubernetes/manifests/metrics-server-de.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: metrics-server
        namespace: kube-system
        labels:
          k8s-app: metrics-server
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        selector:
          matchLabels:
            k8s-app: metrics-server
        template:
          metadata:
            name: metrics-server
            labels:
              k8s-app: metrics-server
          spec:
            priorityClassName: system-cluster-critical
            serviceAccountName: metrics-server
            containers:
            - name: metrics-server
              image: {{ .MetricsServerImage.RepoWithTag }}
              imagePullPolicy: Always
              command:
              - /metrics-server
              - --logtostderr
              - --v=2
              - --kubelet-insecure-tls
              - --requestheader-client-ca-file=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              - --requestheader-username-headers=X-Remote-User
              - --requestheader-group-headers=X-Remote-Group
              - --requestheader-extra-headers-prefix=X-Remote-Extra
              resources:
                limits:
                  cpu: 500m
                  memory: 512Mi
                requests:
                  cpu: 80m
                  memory: 200Mi
              volumeMounts:
              - name: tmp-dir
                mountPath: /tmp
            volumes:
            - name: tmp-dir
              emptyDir: {}

  - path: /srv/kubernetes/manifests/metrics-server-apisvc.yaml
    content: |
      apiVersion: apiregistration.k8s.io/v1beta1
      kind: APIService
      metadata:
        name: v1beta1.metrics.k8s.io
      spec:
        service:
          name: metrics-server
          namespace: kube-system
        group: metrics.k8s.io
        version: v1beta1
        insecureSkipTLSVerify: true
        groupPriorityMinimum: 100
        versionPriority: 100

  - path: /srv/kubernetes/manifests/metrics-server-svc.yaml
    content: |
      apiVersion: v1
      kind: Service
      metadata:
        name: metrics-server
        namespace: kube-system
        labels:
          kubernetes.io/name: "Metrics-server"
          kubernetes.io/cluster-service: "true"
      spec:
        selector:
          k8s-app: metrics-server
        ports:
        - name: https
          port: 443
          protocol: TCP
          targetPort: 443

  - path: /srv/kubernetes/manifests/tiller.yaml
    content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
        name: tiller-deploy
        namespace: kube-system
      spec:
        selector:
          matchLabels:
            app: helm
            name: tiller
        strategy: {}
        template:
          metadata:
            creationTimestamp: null
            labels:
              app: helm
              name: tiller
            # Addition to the default tiller deployment for prioritizing tiller over other non-critical pods with rescheduler
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            serviceAccountName: tiller
            priorityClassName: system-cluster-critical
            tolerations:
            # Additions to the default tiller deployment for allowing to schedule tiller onto controller nodes
            # so that helm can be used to install pods running only on controller nodes
            - key: "node.kubernetes.io/role"
              operator: "Equal"
              value: "master"
              effect: "NoSchedule"
            - key: "CriticalAddonsOnly"
              operator: "Exists"
            containers:
            - env:
              - name: TILLER_NAMESPACE
                value: kube-system
              image: {{.TillerImage.RepoWithTag}}
              imagePullPolicy: IfNotPresent
              livenessProbe:
                httpGet:
                  path: /liveness
                  port: 44135
                initialDelaySeconds: 1
                timeoutSeconds: 1
              name: tiller
              ports:
              - containerPort: 44134
                name: tiller
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 44135
                initialDelaySeconds: 1
                timeoutSeconds: 1
              resources: {}
            nodeSelector:
              beta.kubernetes.io/os: linux
      status: {}
      ---
      apiVersion: v1
      kind: Service
      metadata:
        creationTimestamp: null
        labels:
          app: helm
          name: tiller
        name: tiller-deploy
        namespace: kube-system
      spec:
        ports:
        - name: tiller
          port: 44134
          targetPort: tiller
        selector:
          app: helm
          name: tiller
        type: ClusterIP
      status:
        loadBalancer: {}

  - path: /srv/kubernetes/manifests/tiller-rbac.yaml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: tiller
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: tiller
      subjects:
      - kind: ServiceAccount
        name: tiller
        namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: rbac.authorization.k8s.io

  - path: {{.KubernetesManifestPlugin.ManifestListFile.Path}}
    encoding: gzip+base64
    content: {{.KubernetesManifestPlugin.ManifestListFile.Content.ToGzip.ToBase64}}

  - path: {{.HelmReleasePlugin.ReleaseListFile.Path}}
    encoding: gzip+base64
    content: {{.HelmReleasePlugin.ReleaseListFile.Content.ToGzip.ToBase64}}


{{ range $r := .HelmReleasePlugin.Releases }}
{{ $f := $r.ReleaseFile }}
  - path: {{$f.Path}}
    encoding: gzip+base64
    content: {{$f.Content.ToGzip.ToBase64}}
{{ $f := $r.ValuesFile }}
  - path: {{$f.Path}}
    encoding: gzip+base64
    content: {{$f.Content.ToGzip.ToBase64}}
{{ end }}

  - path: /etc/kubernetes/auth/kubelet-tls-bootstrap-token.tmp{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.TLSBootstrapToken}}

{{ if .AssetsConfig.HasAuthTokens }}
  - path: /etc/kubernetes/auth/tokens.csv.tmp{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.AuthTokens}}
{{ end }}

{{ if .ManageCertificates }}
  - path: /etc/kubernetes/ssl/ca.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.CACert}}

  - path: /etc/kubernetes/ssl/worker-ca-key.pem.enc
    encoding: gzip+base64
    content: {{.AssetsConfig.WorkerCAKey}}

  - path: /etc/kubernetes/ssl/worker-ca.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.WorkerCACert}}

{{- if checkVersion ">= 1.14" .K8sVer }}
  - path: /etc/kubernetes/kubeconfig/worker-bootstrap.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: tls-bootstrap
        user:
          token: $KUBELET_BOOTSTRAP_TOKEN
      contexts:
      - context:
          cluster: local
          user: tls-bootstrap
        name: tls-bootstrap-context
      current-context: tls-bootstrap-context

  - path: /etc/kubernetes/kubeconfig/kubelet.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/kubelet-client-current.pem
          client-key: /etc/kubernetes/ssl/kubelet.key
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
{{- else }}
  - path: /etc/kubernetes/kubeconfig/kubelet.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
{{- end }}

  - path: /etc/kubernetes/ssl/worker.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.WorkerCert}}

  - path: /etc/kubernetes/ssl/worker-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.WorkerKey}}

  - path: /etc/kubernetes/ssl/apiserver.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.APIServerCert}}

  - path: /etc/kubernetes/ssl/apiserver-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.APIServerKey}}

  - path: /etc/kubernetes/ssl/kube-controller-manager.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.KubeControllerManagerCert}}

  - path: /etc/kubernetes/ssl/kube-controller-manager-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.KubeControllerManagerKey}}

  - path: /etc/kubernetes/ssl/kube-scheduler.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.KubeSchedulerCert}}

  - path: /etc/kubernetes/ssl/kube-scheduler-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.KubeSchedulerKey}}

  - path: /etc/kubernetes/ssl/etcd-client.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.EtcdClientCert}}

  - path: /etc/kubernetes/ssl/etcd-client-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.EtcdClientKey}}

  - path: /etc/kubernetes/ssl/etcd-trusted-ca.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.EtcdTrustedCA}}

  - path: /etc/kubernetes/ssl/service-account-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.ServiceAccountKey}}

  - path: /etc/kubernetes/ssl/admin.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.AdminCert}}

  - path: /etc/kubernetes/ssl/admin-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.AdminKey}}

  {{ if .Addons.APIServerAggregator.Enabled -}}
  - path: /etc/kubernetes/ssl/apiserver-aggregator.pem
    encoding: gzip+base64
    content: {{.AssetsConfig.APIServerAggregatorCert}}

  - path: /etc/kubernetes/ssl/apiserver-aggregator-key.pem{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.APIServerAggregatorKey}}
  {{ end -}}
{{ end }}

{{ if .Kubernetes.EncryptionAtRest.Enabled }}
  - path: /etc/kubernetes/additional-configs/encryption-config.yaml{{if .AssetsEncryptionEnabled}}.enc{{end}}
    encoding: gzip+base64
    content: {{.AssetsConfig.EncryptionConfig}}
{{ end }}

  # File needed on every node (used by the kube-proxy DaemonSet), including controllers
  - path: /etc/kubernetes/kubeconfig/kube-proxy.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          server: https://127.0.0.1
      users:
      - name: kube-proxy
        user:
          tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      contexts:
      - context:
          cluster: local
          user: kube-proxy
        name: kube-proxy-context
      current-context: kube-proxy-context

  - path: /etc/kubernetes/kubeconfig/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: kube-controller-manager
        user:
          client-certificate: /etc/kubernetes/ssl/kube-controller-manager.pem
          client-key: /etc/kubernetes/ssl/kube-controller-manager-key.pem
      contexts:
      - context:
          cluster: local
          user: kube-controller-manager
        name: kube-controller-manager-context
      current-context: kube-controller-manager-context

  - path: /etc/kubernetes/kubeconfig/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: kube-scheduler
        user:
          client-certificate: /etc/kubernetes/ssl/kube-scheduler.pem
          client-key: /etc/kubernetes/ssl/kube-scheduler-key.pem
      contexts:
      - context:
          cluster: local
          user: kube-scheduler
        name: kube-scheduler-context
      current-context: kube-scheduler-context

  - path: /etc/kubernetes/kubeconfig/admin.yaml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
          server: https://127.0.0.1
      users:
      - name: admin
        user:
          client-certificate: /etc/kubernetes/ssl/admin.pem
          client-key: /etc/kubernetes/ssl/admin-key.pem
      contexts:
      - context:
          cluster: local
          user: admin
        name: admin-context
      current-context: admin-context

# AdvancedAuditing is enabled by default since K8S v1.8.
# With AdvancedAuditing, you have to provide a audit policy file.
# Otherwise no audit logs are recorded at all.
{{if .Experimental.AuditLog.Enabled -}}
  # Refer to the audit profile used by GCE
  # https://github.com/kubernetes/kubernetes/blob/v1.8.3/cluster/gce/gci/configure-helper.sh#L517
  - path: /etc/kubernetes/apiserver/audit-policy.yaml
    owner: root:root
    permissions: 0600
    content: |
      apiVersion: audit.k8s.io/v1beta1
      kind: Policy
      rules:
        # The following requests were manually identified as high-volume and low-risk,
        # so drop them.
        - level: None
          users: ["system:kube-proxy"]
          verbs: ["watch"]
          resources:
            - group: "" # core
              resources: ["endpoints", "services", "services/status"]
        - level: None
          # Ingress controller reads `configmaps/ingress-uid` through the unsecured port.
          # TODO(#46983): Change this to the ingress controller service account.
          users: ["system:unsecured"]
          namespaces: ["kube-system"]
          verbs: ["get"]
          resources:
            - group: "" # core
              resources: ["configmaps"]
        - level: None
          users: ["kubelet"] # legacy kubelet identity
          verbs: ["get"]
          resources:
            - group: "" # core
              resources: ["nodes", "nodes/status"]
        - level: None
          userGroups: ["system:nodes"]
          verbs: ["get"]
          resources:
            - group: "" # core
              resources: ["nodes", "nodes/status"]
        - level: None
          users:
            - system:kube-controller-manager
            - system:kube-scheduler
            - system:serviceaccount:kube-system:endpoint-controller
          verbs: ["get", "update"]
          namespaces: ["kube-system"]
          resources:
            - group: "" # core
              resources: ["endpoints"]
        - level: None
          users: ["system:apiserver"]
          verbs: ["get"]
          resources:
            - group: "" # core
              resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
        # Don't log HPA fetching metrics.
        - level: None
          users:
            - system:kube-controller-manager
          verbs: ["get", "list"]
          resources:
            - group: "metrics.k8s.io"
        # Don't log these read-only URLs.
        - level: None
          nonResourceURLs:
            - /healthz*
            - /version
            - /swagger*
        # Don't log events requests.
        - level: None
          resources:
            - group: "" # core
              resources: ["events"]
        # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
        # so only log at the Metadata level.
        - level: Metadata
          resources:
            - group: "" # core
              resources: ["secrets", "configmaps"]
            - group: authentication.k8s.io
              resources: ["tokenreviews"]
          omitStages:
            - "RequestReceived"
        # Get responses can be large; skip them.
        - level: Request
          verbs: ["get", "list", "watch"]
          resources:
            - group: "" # core
            - group: "admissionregistration.k8s.io"
            - group: "apiextensions.k8s.io"
            - group: "apiregistration.k8s.io"
            - group: "apps"
            - group: "authentication.k8s.io"
            - group: "authorization.k8s.io"
            - group: "autoscaling"
            - group: "batch"
            - group: "certificates.k8s.io"
            - group: "extensions"
            - group: "metrics.k8s.io"
            - group: "networking.k8s.io"
            - group: "policy"
            - group: "rbac.authorization.k8s.io"
            - group: "settings.k8s.io"
            - group: "storage.k8s.io"
          omitStages:
            - "RequestReceived"
        # Default level for known APIs
        - level: RequestResponse
          resources:
            - group: "" # core
            - group: "admissionregistration.k8s.io"
            - group: "apiextensions.k8s.io"
            - group: "apiregistration.k8s.io"
            - group: "apps"
            - group: "authentication.k8s.io"
            - group: "authorization.k8s.io"
            - group: "autoscaling"
            - group: "batch"
            - group: "certificates.k8s.io"
            - group: "extensions"
            - group: "metrics.k8s.io"
            - group: "networking.k8s.io"
            - group: "policy"
            - group: "rbac.authorization.k8s.io"
            - group: "settings.k8s.io"
            - group: "storage.k8s.io"
          omitStages:
            - "RequestReceived"
        # Default level for all other requests.
        - level: Metadata
          omitStages:
            - "RequestReceived"
{{ end -}}

{{if .Experimental.Authentication.Webhook.Enabled}}
  - path: /etc/kubernetes/webhooks/authentication.yaml
    encoding: base64
    content: {{ .Experimental.Authentication.Webhook.Config }}
{{ end }}

{{ if .SharedPersistentVolume }}
  - path: /opt/bin/load-efs-pv
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      docker run --rm --net=host \
        -v /etc/kubernetes:/etc/kubernetes \
        -v /etc/resolv.conf:/etc/resolv.conf \
        {{ .HyperkubeImage.RepoWithTag }} /bin/bash \
          -vxec \
          'echo "Starting Loading EFS Persistent Volume"
           /kubectl create --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml -f /etc/kubernetes/efs-pv.yaml
           echo "Finished Loading EFS Persistent Volume"'

{{ end }}

{{if .Kubernetes.Networking.AmazonVPC.Enabled}}
  - path: /opt/bin/aws-k8s-cni-max-pods
    owner: root:root
    permissions: 0755
    encoding: gzip+base64
    content: {{.Kubernetes.Networking.AmazonVPC.MaxPodsScript.ToGzip.ToBase64}}

  - path: /srv/kubernetes/manifests/aws-k8s-cni.yaml
    content: |
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      # kubernetes versions before 1.8.0 should use rbac.authorization.k8s.io/v1beta1
      kind: ClusterRole
      metadata:
        name: aws-node
      rules:
      - apiGroups:
        - crd.k8s.amazonaws.com
        resources:
        - "*"
        - namespaces
        verbs:
        - "*"
      - apiGroups: [""]
        resources:
        - pods
        - nodes
        - namespaces
        verbs: ["list", "watch", "get"]
      - apiGroups: ["extensions"]
        resources:
        - daemonsets
        verbs: ["list", "watch"]
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: aws-node
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      # kubernetes versions before 1.8.0 should use rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: aws-node
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: aws-node
      subjects:
      - kind: ServiceAccount
        name: aws-node
        namespace: kube-system
      ---
      kind: DaemonSet
      apiVersion: apps/v1
      metadata:
        name: aws-node
        namespace: kube-system
        labels:
          k8s-app: aws-node
      spec:
        updateStrategy:
          type: RollingUpdate
        selector:
          matchLabels:
            k8s-app: aws-node
        template:
          metadata:
            labels:
              k8s-app: aws-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            priorityClassName: system-node-critical
            serviceAccountName: aws-node
            hostNetwork: true
            tolerations:
            - operator: Exists
          # required to avoid pod creation errors like the below:
          #  NetworkPlugin cni failed to set up pod "heapster-5ccb7ff4b-cdq24_kube-system" network: failed to find plugin "loopback" in path [/opt/cni/bin]
            initContainers:
            - name: hyperkube
              image: {{ .HyperkubeImage.RepoWithTag }}
              command:
              - /bin/sh
              - -c
              - "cp /opt/cni/bin/* /host/opt/cni/bin/"
              volumeMounts:
              - mountPath: /host/opt/cni/bin
                name: cni-bin-dir
            containers:
            - image: 602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni:1.2.0
              imagePullPolicy: Always
              ports:
              - containerPort: 60000
                name: metrics
              name: aws-node
              env:
                - name: AWS_VPC_K8S_CNI_LOGLEVEL
                  value: DEBUG
                - name: MY_NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: WATCH_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              resources:
                requests:
                  cpu: 10m
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /host/opt/cni/bin
                name: cni-bin-dir
              - mountPath: /host/etc/cni/net.d
                name: cni-net-dir
              - mountPath: /host/var/log
                name: log-dir
              - mountPath: /var/run/docker.sock
                name: dockersock
            volumes:
            - name: cni-bin-dir
              hostPath:
                path: /opt/cni/bin
            - name: cni-net-dir
              hostPath:
                # changed from /etc/cni/net.d to /etc/kubernetes/cni/net.d to accomodate kube-aws' setup
                # original: https://github.com/aws/amazon-vpc-cni-k8s/blob/a91e807c8d752a13f8047f54fa78fcd3a37dfc20/config/v1.1/aws-k8s-cni.yaml#L92
                path: /etc/kubernetes/cni/net.d
            - name: log-dir
              hostPath:
                path: /var/log
            - name: dockersock
              hostPath:
                path: /var/run/docker.sock
      ---
      apiVersion: apiextensions.k8s.io/v1beta1
      kind: CustomResourceDefinition
      metadata:
        name: eniconfigs.crd.k8s.amazonaws.com
      spec:
        scope: Cluster
        group: crd.k8s.amazonaws.com
        version: v1alpha1
        names:
          plural: eniconfigs
          singular: eniconfig
          kind: ENIConfig
{{end}}

{{if .Experimental.GpuSupport.Enabled }}
  - path: /srv/kubernetes/manifests/nvidia-driver-installer.yaml
    content: |
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-driver-installer
        namespace: kube-system
        labels:
          app: nvidia-driver-installer
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        selector:
          matchLabels:
            name: nvidia-driver-installer
        template:
          metadata:
            labels:
              name: nvidia-driver-installer
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            nodeSelector:
              kube-aws.coreos.com/gpu: nvidia
            priorityClassName: system-node-critical
            hostNetwork: true
            hostPID: true
            volumes:
            - name: dev
              hostPath:
                path: /dev
            - name: nvidia-install-dir-host
              hostPath:
                path: /opt/nvidia
            - name: root-mount
              hostPath:
                path: /
            initContainers:
            - image: "{{.Experimental.GpuSupport.InstallImage}}"
              name: nvidia-driver-installer
              resources:
                requests:
                  cpu: 0.15
              securityContext:
                privileged: true
              env:
                - name: NVIDIA_INSTALL_DIR_HOST
                  value: /opt/nvidia
                - name: NVIDIA_INSTALL_DIR_CONTAINER
                  value: /usr/local/nvidia
                - name: ROOT_MOUNT_DIR
                  value: /root
                - name: NVIDIA_DRIVER_VERSION
                  value: "{{.Experimental.GpuSupport.Version}}"
              volumeMounts:
              - name: nvidia-install-dir-host
                mountPath: /usr/local/nvidia
              - name: dev
                mountPath: /dev
              - name: root-mount
                mountPath: /root
            containers:
            - image: "gcr.io/google-containers/pause:2.0"
              name: pause
      ---

      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: nvidia-gpu-device-plugin
        namespace: kube-system
        labels:
          k8s-app: nvidia-gpu-device-plugin
          addonmanager.kubernetes.io/mode: Reconcile
      spec:
        updateStrategy:
          rollingUpdate:
            maxUnavailable: 100%
          type: RollingUpdate
        selector:
          matchLabels:
            k8s-app: nvidia-gpu-device-plugin
        template:
          metadata:
            labels:
              k8s-app: nvidia-gpu-device-plugin
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
          spec:
            nodeSelector:
              kube-aws.coreos.com/gpu: nvidia
            priorityClassName: system-node-critical
            hostNetwork: true
            hostPID: true
            volumes:
            - name: device-plugin
              hostPath:
                path: /var/lib/kubelet/device-plugins
            - name: dev
              hostPath:
                path: /dev
            containers:
            - image: "k8s.gcr.io/nvidia-gpu-device-plugin@sha256:0842734032018be107fa2490c98156992911e3e1f2a21e059ff0105b07dd8e9e"
              command: ["/usr/bin/nvidia-gpu-device-plugin", "-logtostderr", "-host-path=/opt/nvidia"]
              name: nvidia-gpu-device-plugin
              resources:
                requests:
                  cpu: 50m
                  memory: 10Mi
                limits:
                  cpu: 50m
                  memory: 10Mi
              securityContext:
                privileged: true
              volumeMounts:
              - name: device-plugin
                mountPath: /device-plugin
              - name: dev
                mountPath: /dev
{{end}}

  # Without the namespace annotation the pod will be unable to assume any roles
  - path: /srv/kubernetes/manifests/kube-system-ns.yaml
    content: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: kube-system
        annotations:
          iam.amazonaws.com/permitted: ".*"
        labels:
	{{- range $key, $value := .KubeSystemNamespaceLabels }}
          {{ $key }}: {{ $value }}
	{{- end }}

  - path: /opt/bin/retry
    owner: root:root
    permissions: 0755
    content: |
      #!/bin/bash
      max_attempts="$1"; shift
      cmd="$@"
      attempt_num=1
      attempt_interval_sec=3

      until $cmd
      do
          if (( attempt_num == max_attempts ))
          then
              echo "Attempt $attempt_num failed and there are no more attempts left!"
              exit 1
          else
              echo "Attempt $attempt_num failed! Trying again in $attempt_interval_sec seconds..."
              ((attempt_num++))
              sleep $attempt_interval_sec;
          fi
      done

  - path: /opt/bin/handle-disable-request
    permissions: 0755
    content: |
      #!/bin/bash
      # Allows a controller to disable its core services upon request
      # Created to allow more ambitious kubernetes upgrades
      # and changes such as changing cluster settings such as service_cidr or pod_cidr
      #
      # A request to disable is a configmap matching the hostname and kubernetes version containing a list of core service to stop: -
      # apiVersion: v1
      # kind: ConfigMap
      # metadata:
      #   name: kube-aws-migration-disable-ip-10-29-26-83.us-west-2.compute.internal
      #   namespace: kube-system
      # data:
      #   kubernetesVersion: v1.9.3
      #   disable: "kube-apiserver kube-controller-manager kube-scheduler"

      retries=5
      hyperkube_image="{{.HyperkubeImage.RepoWithTag}}"
      my_kubernetes_version="{{.HyperkubeImage.Tag}}"
      myhostname=$(hostname -f)
      disable_confmap_name="kube-aws-migration-disable-${myhostname}"
      valid_services="kube-apiserver kube-controller-manager kube-scheduler"

      kubectl() {
        local tries=0
        local result_text=""
        local return_code=0

        while [ "$tries" -lt "$retries" ]; do
          result_text=$(docker run --rm -i --net=host -v /tmp:/tmp:rw -v /etc/kubernetes:/etc/kubernetes:ro  -v /etc/resolv.conf:/etc/resolv.conf:ro $hyperkube_image /kubectl --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml "$@")
          return_code=$?
          if [ "$return_code" -eq "0" ]; then
            echo "${result_text}"
            break
          fi
          sleep 10
          tries=$((tries+1))
        done
        return $return_code
      }

      log() {
        echo "$@" >&2
      }

      get_disable_request() {
        kubectl get cm -n kube-system $disable_confmap_name -o json --ignore-not-found
      }

      valid_disable_request() {
        local disable_payload=$1

        if [[ -n "${disable_payload}" ]]; then
          log "found a disable request"
          local kubernetes_version=$(echo ${disable_payload} | jq -er '.data.kubernetesVersion')
          if [[ "${kubernetes_version}" == "${my_kubernetes_version}" ]]; then
            log "valid request: kubernetes version match: ${kubernetes_version}"
            return 0
          else
            log "invalid request: kubernetes version ${kubernetes_version} does not match my version ${my_kubernetes_version}"
            return 1
          fi
        fi
        log "no disable request found"
        return 1
      }

      valid_service() {
        for s in $valid_services; do
          if [[ "$s" == $1 ]]; then
            return 0
          fi
        done
        return 1
      }

      disable_service() {
        local service=$1

        if [[ -f "/etc/kubernetes/manifests/${service}.yaml" ]]; then
          log "Moving manifest /etc/kubernetes/manifests/${service}.yaml to /etc/kubernetes/${service}.yaml"
          mv /etc/kubernetes/manifests/${service}.yaml /etc/kubernetes/${service}.yaml
        else
          log "No manifest found when looking for /etc/kubernetes/manifests/${service}.yaml"
        fi

        local container=$(docker ps | grep "k8s_${service}" | awk '{print $1}')
        if [[ -n "${container}" ]]; then
          log "stopping ${service} container ${container}..."
          docker stop $container && docker rm $container
        else
          log "no docker container found matching k8s_${service}"
        fi
      }

      # MAIN

      log "Running watcher for requests to disable core services..."
      while true
      do
        log "checking disable request kube-system/${disable_confmap_name} ..."
        request=$(get_disable_request)
        if valid_disable_request "${request}"; then
          log "I've received a valid disable request!"
          disable=$(echo "${request}" | jq -erc '.data.disable')
          for d in ${disable}; do
            log "disabling $d..."
            if valid_service $d; then
              disable_service $d
            else
              log "ERROR: service %d is not valid - valid services are ${valid_services}"
            fi
          done
        else
          log "no request to disable services found"
        fi

        sleep 10
      done

  - path: /opt/bin/handle-cluster-cidr-changes
    permissions: 0755
    content: |
      #!/bin/bash
      # Script handles cluster coming up with changed podcidr and servicecidr

      podcidr="{{ .PodCIDR }}"
      servicecidr="{{ .ServiceCIDR }}"
      retries=5
      job_timeout_seconds=120
      cidr_confmap_name="saved-kube-cluster-cidrs"
      hyperkube_image="{{.HyperkubeImage.RepoWithTag}}"
      myhostname=$(hostname -f)
      deleted_myself=0

      kubectl() {
        local tries=0
        local result_text=""
        local return_code=0

        while [ "$tries" -lt "$retries" ]; do
          result_text=$(docker run --rm -i --net=host -v /tmp:/tmp:rw -v /etc/kubernetes:/etc/kubernetes:ro  -v /etc/resolv.conf:/etc/resolv.conf:ro $hyperkube_image /kubectl --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml "$@")
          return_code=$?
          if [ "$return_code" -eq "0" ]; then
            echo "${result_text}"
            break
          fi
          sleep 10
          tries=$((tries+1))
        done
        return $return_code
      }

      log() {
        echo "$@" >&2
      }

      # cidr2mask takes a CIDR bit value b in range (0 - 32) and returns a netmask with b bits set to 1,
      # counting from the most significant bits left to right.
      # The implementation below splits to calculation of the mask into the 4 dotted octets.
      # By dividing the CIDR number b by 8 tells us: -
      #   1. How many octets will be FULL, i.e. contains all 1's
      #   2. Which octet is PARTIAL, i.e. holds the change between 1's to 0's
      #   3. One or more octets are EMPTY, i.e. that contain all 0's.
      # e.g. a CIDR of 18, will have two full octets of 255, one with the left-over two bits, and then a
      # trailing octet of 0's: -
      # Example produce netmask for cidr /18 (b = 18)
      #           ------> count ----->
      # Bits:     11111111 11111111 11000000 000000
      #           FULL     FULL     PARTIAL  EMPTY
      # Result:   255      255      192      0

      cidr2mask() {
        local b=$1
        local i mask=""
        local full_octets=$(($b/8))
        local partial_octet=$(($b%8))

        for ((i=0;i<4;i+=1)); do
          if [ $i -lt $full_octets ]; then
            # This octet must be full 1's
            mask+=255
          elif [ $i -eq $full_octets ]; then
            # The edge of the mask falls within this octet.
            # To work out the integer value for the set bits we can subtract 2 raised to the power of the
            # bit column where the 0's begin.
            mask+=$((256 - 2**(8-$partial_octet)))
          else
            # This octet must be full 0's
            mask+=0
          fi
          test $i -lt 3 && mask+=.
        done

        echo $mask
      }

      # ip2network takes an ip address in dotted notation and netmask both in dotted notation, e.g.
      # 192.168.0.3 & 255.255.0.0.  It applies a bitwise AND of IP with the netmask to get the network
      # address.
      ip2network() {
        local ip=$1
        local mask=$2
        local i1 i2 i3 i4
        local m1 m2 m3 m4

        { IFS=. read i1 i2 i3 i4; } <<< $ip
        { IFS=. read m1 m2 m3 m4; } <<< $mask
        printf "%d.%d.%d.%d" "$((i1 & m1))" "$((i2 & m2))" "$((i3 & m3))" "$((i4 & m4))"
      }

      # IPwithinCIDR returns true if the provided ip falls within the network cidr.
      # To work this out we can apply the network mask from the network cidr to the ip address
      # to see to get its network and then compare this with the network cidr.

      # e.g.  Is 10.66.250.5 in network 10.66.128.0/18?
      # network mask from CIDR /18 = 11111111 11111111 11000000 000000 = 255.255.192.0
      # The network part of the range cidr is 10.66.128.0
      # Network    10       66       128      0
      #  in binary 00001010 01000010 10000000 00000000
      # Mask       11111111 11111111 11000000 00000000
      # AND result 00001010 01000010 10000000 00000000
      #  in dotted 10       66       128      0
      # So 10.66.128.0/18 describes the network 10.66.128.0 (as expected)
      # Applying the same network mask to the ip address: -
      # IP Address 10       66       250      5
      #  in binary 00001010 01000010 11111010 00000101
      # Mask       11111111 11111111 11000000 00000000
      # AND result 00001010 01000010 10000000 00000000
      #  in dotted 10       66       128      0
      # 10.66.250.5 DOES fall within the network 10.66.128.0/18 because it has the SAME network number.
      # In another example does IP 10.66.126.99 also fall in the 10.66.100.8/18 network?
      # IP Address 10       66       126      99
      #  in binary 00001010 01000010 01111110 01100011
      # Mask       11111111 11111111 11000000 00000000
      # AND result 00001010 01000010 01000000 00000000
      #  in dotted 10       66       64       0
      # 10.66.126.99 DOES NOT fall within the network because after applying the mask the network number
      # is different.

      IPwithinCIDR() {
        local ip=$1
        local network_cidr=$2

        local net_mask=$(cidr2mask ${network_cidr##*/})
        # Normalise/correct network CIDR by applying its own network mask to it (e.g 66.66.0.0/10 is actually net 66.64.0.0/10)
        local net_num=$(ip2network ${network_cidr%%/*} $net_mask)
        local ip_net=$(ip2network $ip $net_mask)

        [[ "${ip_net}" == "${net_num}" ]]
      }

      # ip2int takes a single ip address in dotted notation and coverts it into an integer.
      # ip2int and int2ip are used for properly calculating the 1st address in a subnet (+1).

      # For each of the 4 ip octets (each an 8 bit value in decimal) we take the value and then bit shift
      # 8 bits to make room for the next octet which is OR'd into the empty space created by the shift.
      # In the ipaddress a.b.c.d
      # 'a' ends up being shifted 24 bits to the left
      # 'b' ends up being shifted 16 bits to the left
      # 'c' is shifted 8 bits to the left
      # 'd' is not shifted
      # e.g. 192.168.0.3 = 3232235523 (decimal) - the binary representation of this makes it more clear: -
      # binary : 11000000 10101000 00000000 00000011
      # decimal: 192      168      0        3

      ip2int()
      {
          local a b c d
          { IFS=. read a b c d; } <<< $1
          echo $(((((((a << 8) | b) << 8) | c) << 8) | d))
      }

      # int2ip takes an integer representation of an ipaddress (such as those produced by ip2int) and
      # returns a dotted ip address that everyone recognises.  It does this by AND'ing the ip variable
      # with 0xff (11111111 in binary) to get the last octet.  Then it bit shifts 8 places to
      # the right which places the next octet in the last 8 bits.  If we do this
      # 4 times then we end up with an dotted ip string in the conventional dotted notation.
      # e.g. let's convert 3232235523 back into ip...
      # (get last 8 bits) 11000000 10101000 00000000 00000011 & 00000000 0000000 0000000 11111111 = 00000000 0000000 0000000 00000011 = ip "3"
      # (shift 8 right) 11000000 10101000 00000000 00000011 >> 8 = 11000000 10101000 00000000
      # (get last 8 bits) 11000000 10101000 00000000 & 0000000 0000000 11111111 = 0000000 0000000 00000000 = ip "0.3"
      # (shift 8 right) 11000000 10101000 00000000 >> 8 = 11000000 10101000
      # (get last 8 bits) 11000000 10101000 & 0000000 11111111 = 0000000 10101000 = ip "168.0.3"
      # (shift 8 bits right ) 11000000 10101000 >> 8 = 11000000
      # (get last 8 bits) 11000000 & 11111111 = 11000000 = ip "192.168.0.3"

      int2ip()
      {
          local ui32=$1; shift
          local ip n
          ip=$((ui32 & 0xff))
          ui32=$((ui32 >> 8))
          for n in 1 2 3; do
              ip="$((ui32 & 0xff)).${ip}"
              ui32=$((ui32 >> 8))
          done
          echo $ip
      }

      get_cidr_confmap() {
        kubectl get cm -n kube-system $cidr_confmap_name -o json --ignore-not-found
      }

      save_configmap() {
        local check

        log "Saving pod and service cidrs to configmap kube-system/${cidr_confmap_name}"
        if ! check=$(kubectl -n kube-system get cm $cidr_confmap_name --no-headers --ignore-not-found | awk '{print $1}'); then
          log "Could not access kubernetes api - can't save configmap."
          return 1
        fi
        if [[ -n "${check}" ]]; then
          log "Configmap ${cidr_confmap_name} already exists, skipping save."
          return 1
        fi
        kubectl -n kube-system create configmap $cidr_confmap_name --from-literal=lock=$myhostname --from-literal=podCIDR=$podcidr --from-literal=serviceCIDR=$servicecidr
      }

      i_have_lock() {
        local cm_json lock_host
        if ! cm_json=$(get_cidr_confmap); then
          log "Failed to read lock configmap ${cidr_confmap_name}, skipping."
          return 1
        fi
        lock_host=$(echo ${cm_json} | jq -er '.data.lock')
        if [[ "${lock_host}" != "${myhostname}" ]]; then
          log "I don't have the lock"
          return 1
        fi
        return 0
      }

      delete_configmap() {
        local cm

        local started_time=$(date +%s)
        if kubectl -n kube-system delete cm $cidr_confmap_name; then
          cm=$(get_cidr_confmap)
          while [[ "${cm}" != "" ]]; do
            if [ "$(date +%s)" -gt "$((started_time+job_timeout_seconds))" ]; then
              log "Timed out waiting for deletion of configmap!"
              return 1
            fi
            log "Waiting for deletion of configmap ${cidr_confmap_name}..."
            sleep 10
          done
        fi
        return 0
      }

      list_all_services() {
          kubectl get service --all-namespaces --no-headers --ignore-not-found | awk '{printf "%s:%s:%s\n", $1, $2, $4}' || log "Failed to list services" && exit 1
      }

      list_all_pods() {
          kubectl get pods --all-namespaces --no-headers --ignore-not-found -o wide | awk '{printf "%s:%s:%s:%s\n", $1, $2, $7, $8}' || log "Failed to list pods" && exit 1
      }

      list_all_nodes() {
        kubectl get nodes --no-headers --ignore-not-found | awk '{printf "%s:%s\n", $1, $3}'
      }

      read_ns() {
          echo $1 | awk -F: '{print $1}'
      }

      read_name() {
          echo $1 | awk -F: '{print $2}'
      }

      read_ip() {
          echo $1 | awk -F: '{print $3}'
      }

      read_hostname() {
        echo $1 | awk -F: '{print $4}'
      }

      # read_node - specifically reads and returns a node's podCIDR value
      read_node() {
        local node=$1

        kubectl get node $node -o json --ignore-not-found | jq -r '.spec.podCIDR'
      }

      delete_node() {
        local node=$1

        [[ "${node}" == "${myhostname}" ]] && deleted_myself=1
        kubectl delete node "${node}"
      }

      pod_exists() {
        local ns=$1
        local name=$2
        local found

        if found=$(kubectl -n $ns get pod $name --no-headers --ignore-not-found); then
          [[ -n "${found}" ]] && return 0
        fi
        return 1
      }

      delete_pod() {
        local pod_ns=$1
        local pod_name=$2
        local check

        log "Deleting pod ${pod_ns}/${pod_name}..."
        kubectl -n $pod_ns delete pod $pod_name --ignore-not-found --grace-period=-1 --force
      }

      find_node_cidr() {
        local node=$1
        local found

        found=$(kubectl get pods --all-namespaces --no-headers --ignore-not-found --field-selector=spec.nodeName=$node -o=custom-columns=PODIP:.status.podIP,HOSTIP:.status.hostIP | awk '($1 != $2){print $1}' | head -1)
        if [[ -z "$found" ]]; then
          log "Found no running pods on node ${node} that we can use to determine the podCIDR"
          return 1
        fi
        echo "${found%\.*}.0/24"
      }

      # recreate_service - it is impossible to patch a kubernetes service to remove its service ip address and so we take a copy of it
      # filtering the address and a couple of metadata fields and the force apply which will do a delete followed by an add.
      recreate_service() {
        local ns=$1
        local service=$2

        local tmpfile=$(mktemp /tmp/recreate-service.XXXXXX)
        kubectl -n $ns get svc $service --export -o yaml >${tmpfile}
        kubectl -n $ns delete svc $service
        kubectl -n $ns create -f ${tmpfile}
        rm -f ${tmpfile}
      }

      # stop a controller by writing a special kube-aws disable service configmap
      disable_controller() {
        local controller=$1
        local version=$2

        local request="$(cat <<EOT
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-aws-migration-disable-${controller}
        namespace: kube-system
      data:
        kubernetesVersion: ${version}
        disable: "kube-controller-manager"
      EOT
      )"

        log "Creating disable service configmap kube-system/kube-aws-migration-disable-${controller}"
        echo "${request}" | kubectl -n kube-system create -f - || return 1
        return 0
      }

      node_version() {
        local node=$1
        kubectl get node $node --no-headers --ignore-not-found | awk '{print $5}'
      }

      # serviceCIDRmatch - looks at a nodes labels for a service-cidr label that matches the current known servicecidr.
      # We want to identify controllers/masters that could be running a kube-controller-manager that needs to be stopped.
      serviceCIDRmatch() {
        local node=$1
        local labels

        labels=$(kubectl get node --no-headers --ignore-not-found --show-labels "${node}" | awk '{print $6}')
        echo $labels | grep "service-cidr=${servicecidr/\//_}" >/dev/null 2>&1
      }

      # validate_nodes - will stop the kube-controller on masters if the service cidr OR pod cidr has changed.
      # It will delete all nodes with a nodecidr that lies outside of the podcidr range (after stopping the kube controller in
      # the case of masters).
      validate_nodes() {
        local nodecidr nodes nodepair node node_type action_stop_controller action_delete_node
        log "*** Validating Nodes ***"
        if ! i_have_lock; then
          log "I don't have the lock so won't validate nodes"
          return 0
        fi
        nodes=$(list_all_nodes) || (log "Failed to list nodes" && return 1)
        [[ -z "${nodes}" ]] && log "No nodes found." && return 0
        log "Found $(echo "${nodes}" | wc -l) nodes"
        for nodepair in $nodes
        do
          node=${nodepair%%:*}
          node_type=${nodepair##*:}
          action_stop_controller=0
          action_delete_node=0

          if nodecidr=$(read_node $node) && [[ "${nodecidr}" != "null" ]]; then
            log "Node ${node_type}/${node} has podCIDR ${nodecidr} (read from node object)"
          else
            if nodecidr=$(find_node_cidr $node); then
              log "Node ${node_type}/${node} has podCIDR ${nodecidr} (calculated from pods)"
            else
              log "Node ${node_type}/${node} has no podCIDR"
            fi
          fi

          if [[ -n "${nodecidr}" ]] && ! IPwithinCIDR ${nodecidr%%/*} $podcidr; then
            if [[ "${node_type}" == "master" && "${node}" != "${myhostname}" ]]; then
              action_stop_controller=1
            fi
            log "Node ${node_type}/${node} - not ok - (outside podcidr)"
            action_delete_node=1
          else
            log "Node ${node_type}/${node} - ok - (within podcidr)"
          fi

          if [[ "${node_type}" == "master" && "${node}" != "${myhostname}" ]] && ! serviceCIDRmatch ${node}; then
            log "Node ${node_type}/${node} - not ok - (service cidr out-of-range)"
            action_stop_controller=1
          fi

          [[ "${action_stop_controller}" == "1" ]] && disable_controller $node $(node_version $node)
          [[ "${action_delete_node}" == "1" ]] && delete_node $node
        done
      }

      # validate_kube_service - if we change the ServiceCIDR then it is possible for the kubernetes service to not be on the expected ip address
      # We find the desired kubernetes ip by adding +1 to the ServiceCIDR and then we delete the kubernetes service if its IP does not match this.
      # The kubernetes service will always be recreated if it does not exist.
      validate_kube_service() {
        log "*** Validating the 'kubernetes' Service ***"
        if ! i_have_lock; then
          log "I don't have the lock so won't check kubernetes service"
          return 0
        fi
        local wanted_svc_int=$(ip2int ${servicecidr%%/*})
        local wanted_svc_int=$((wanted_svc_int + 1))
        local wanted_svc_ip=$(int2ip $wanted_svc_int)
        local k8svc=$(kubectl get svc kubernetes -n default | grep "^kubernetes " | awk '{print $3}')
        if [[ "${k8svc}" != "${wanted_svc_ip}" ]]; then
          log "Service default/kubernetes - not ok - (${k8svc} is not expected ${wanted_svc_ip})"
          kubectl delete svc kubernetes -n default
        else
          log "Service default/kubernetes - ok - (${k8svc} is expected ip address)"
        fi
      }

      # validate_services - will check all the kubernetes services in the cluster.  Any services it finds with a service ip adderss that
      # lies outside of the ServiceCIDR will be re-created so that they pick up a new valid ip address.
      validate_services() {
        log "*** Validating Services ***"
        if ! i_have_lock; then
          log "I don't have the lock so won't validate services"
          return 0
        fi
        local services=$(list_all_services) || (log "Could not list services" && return 1)
        [[ -z "${services}" ]] && log "No services found." && return 0
        log "Found $(echo "${services}" | wc -l) services"
        for serv in $services; do
          local service_ns=$(read_ns $serv)
          local service_name=$(read_name $serv)
          local service_ip=$(read_ip $serv)
          if [[ -z "${service_ip}" || "${service_ip}" == "None" ]]; then
            log "Service ${service_ns}/${service_name} - ok - (no ip address)"
            continue
          fi
          if ! IPwithinCIDR "${service_ip}" ${servicecidr}; then
            log "Service ${service_ns}/${service_name} - not ok - (outside service cidr)"
            recreate_service ${service_ns} ${service_name}
          else
            log "Service ${service_ns}/${service_name} - ok - (within service cidr)"
          fi
        done
      }

      # validate_pods - we need to remove pods with the wrong ips.
      validate_pods() {
        log "*** Validating Pods ***"
        if ! i_have_lock; then
          log "I don't have the lock so won't validate pods"
          return 0
        fi
        local pods=$(list_all_pods) || (log "Could not list pods" && return 1)
        [[ -z "${pods}" ]] && log "No pods found." && return 0
        log "Found $(echo "${pods}" | wc -l) pods"
        for pod in $pods; do
          local pod_ns=$(read_ns $pod)
          local pod_name=$(read_name $pod)
          local pod_ip=$(read_ip $pod)
          local pod_host=$(read_hostname $pod)
          if [[ -z "${pod_ip}" || "${pod_ip}" == "<none>" ]]; then
            log "Pod ${pod_ns}/${pod_name} - ok - (no ip address)"
            continue
          fi
          if echo $pod_host | grep "${pod_ip/\./-}" >/dev/null; then
            log "Pod ${pod_ns}/${pod_name} - ok - (host network)"
            continue
          fi
          if ! IPwithinCIDR "${pod_ip}" ${podcidr}; then
            log "Pod ${pod_ns}/${pod_name} - not ok - (outside podcidr)"
            delete_pod ${pod_ns} ${pod_name}
          else
            log "Pod ${pod_ns}/${pod_name} - ok - (within podcidr)"
          fi
        done
      }

      # MAIN
      log "Running checks for changed pod or service cidrs..."

      while ! (kubectl get ns kube-system >/dev/null 2>&1); do
        echo "Waiting for apiserver to be available..."
        sleep 3
      done

      log "Current pod cidr: ${podcidr}"
      log "Current service cidr: ${servicecidr}"

      update_cm="false"
      cidr_json=$(get_cidr_confmap)
      if [[ -n "${cidr_json}" ]]; then
        previous_podcidr=$(echo ${cidr_json} | jq -er '.data.podCIDR')
        log "Previous pod cidr: ${previous_podcidr}"
        previous_servicecidr=$(echo ${cidr_json} | jq -er '.data.serviceCIDR')
        log "Previous service cidr: ${previous_servicecidr}"

        if [[ "${previous_podcidr}" == "${podcidr}" && "${previous_servicecidr}" == "${servicecidr}" ]]; then
          log "The pod and service cidrs have not changed, nothing to do."
          exit 0
        fi

        log "Handling CIDR changes..."
        delete_configmap
        save_configmap
        validate_nodes
        validate_pods
        validate_kube_service
        validate_services
      else
        log "No previous cidrs saved ..."
        save_configmap
      fi

      if [[ "${deleted_myself}" == "1" ]]; then
        log "I deleted my own node - restarting kubelet and docker"
        systemctl stop kubelet
        systemctl restart docker
        systemctl start kubelet
      fi

      log "Finished"
      exit 0

  {{if .Experimental.AuditLog.Enabled -}}
  # Check worker communication by searching audit logs
  - path: /opt/bin/check-worker-communication
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      set -ue

      AUDIT_LOG_PATH="{{.Experimental.AuditLog.LogPath}}"

      vols="-v /srv/kubernetes:/srv/kubernetes:ro -v /etc/kubernetes:/etc/kubernetes:ro"
      kubectl() {
        /usr/bin/docker run -i --rm $vols --net=host {{.HyperkubeImage.RepoWithTag}} /kubectl --kubeconfig=/etc/kubernetes/kubeconfig/admin.yaml --request-timeout=1s "$@"
      }

      queryserver() {
        echo "Checking to see if workers are communicating with API server."
        auditlogs | grep -v 127.0.0.1 | grep kubelet | jq -c 'select(.responseStatus.code == 200)' --exit-status
      }

      auditlogs() {
        if [ "$AUDIT_LOG_PATH" == "/dev/stdout" ]; then
          # Let `docker logs` gather logs for periods slightly longer than the delay between `queryserver` calls
          # so that we won't drop any lines.
          docker logs --since 11s ${DOCKERIMAGE} |& cat
        else
          cat "$AUDIT_LOG_PATH"
        fi
      }

      #This checks whether there are any nodes other than controllers. If there is, this indicates it is a cluster update, if there is not, is a fresh cluster.
      #If it is an update, we check whether the workers can communicate with the API server before reporting success.

      kubectl get nodes 2>/dev/null
      RC=$?
      if [[ $RC -gt 0 ]]; then
        echo "No nodes present, assuming API server not yet ready, cannot verify cluster is up."
        exit 1
      fi

      NUM_WORKERS=$(kubectl get nodes -l node.kubernetes.io/role!="master" 2>/dev/null)
      if [[ -z ${NUM_WORKERS} ]]; then
        echo "Fresh cluster, not checking for existing workers."
        exit 0
      else
        #We first retrieve the name of the image which is running the api-server, and then search its logs for any mentions of kubelet with a 200 response.

        DOCKERIMAGE=$(docker ps | grep -i k8s_kube-apiserver_kube-apiserver | awk '{print $1}')
        until queryserver; do echo "Worker communication failed, retrying." && sleep 10; done
        echo "Communication with workers has been established."
      fi
  {{end -}}

  {{if .HostOS.BashPrompt.Enabled -}}
  # Enable informative coreos ssh shell prompts
  - path: /etc/bash/bashrc-kube-aws
    permissions: 0644
    owner: root:root
    content: |
      # /etc/bash/bashrc
      #
      # This file is sourced by all *interactive* bash shells on startup,
      # including some apparently interactive shells such as scp and rcp
      # that can't tolerate any output.  So make sure this doesn't display
      # anything or bad things will happen !


      # Test for an interactive shell.  There is no need to set anything
      # past this point for scp and rcp, and it's important to refrain from
      # outputting anything in those cases.
      if [[ $- != *i* ]] ; then
          # Shell is non-interactive.  Be done now!
          return
      fi

      # Bash won't get SIGWINCH if another process is in the foreground.
      # Enable checkwinsize so that bash will check the terminal size when
      # it regains control.  #65623
      # http://cnswww.cns.cwru.edu/~chet/bash/FAQ (E11)
      shopt -s checkwinsize

      # Disable completion when the input buffer is empty.  i.e. Hitting tab
      # and waiting a long time for bash to expand all of $PATH.
      shopt -s no_empty_cmd_completion

      # Enable history appending instead of overwriting when exiting.  #139609
      shopt -s histappend

      # Save each command to the history file as it's executed.  #517342
      # This does mean sessions get interleaved when reading later on, but this
      # way the history is always up to date.  History is not synced across live
      # sessions though; that is what `history -n` does.
      # Disabled by default due to concerns related to system recovery when $HOME
      # is under duress, or lives somewhere flaky (like NFS).  Constantly syncing
      # the history will halt the shell prompt until it's finished.
      #PROMPT_COMMAND='history -a'

      # Change the window title of X terminals
      case ${TERM} in
      [aEkx]term*|rxvt*|gnome*|konsole*|interix)
          PS1='\[\033]0;{{.HostOS.BashPrompt.ControllerLabel}}{{.HostOS.BashPrompt.Divider}}{{.ClusterName}} \h\007\]'
          ;;
      screen*)
          PS1='\[\033k{{.HostOS.BashPrompt.ControllerLabel}}{{.HostOS.BashPrompt.Divider}}{{.ClusterName}} \h\033\\\]'
          ;;
      *)
          unset PS1
          ;;
      esac

      # Set colorful PS1 only on colorful terminals.
      # dircolors --print-database uses its own built-in database
      # instead of using /etc/DIR_COLORS.  Try to use the external file
      # first to take advantage of user additions.
      # We run dircolors directly due to its changes in file syntax and
      # terminal name patching.
      use_color=false
      if type -P dircolors >/dev/null ; then
          # Enable colors for ls, etc.  Prefer ~/.dir_colors #64489
          LS_COLORS=
          if [[ -f ~/.dir_colors ]] ; then
          eval "$(dircolors -b ~/.dir_colors)"
          elif [[ -f /etc/DIR_COLORS ]] ; then
          eval "$(dircolors -b /etc/DIR_COLORS)"
          else
          eval "$(dircolors -b)"
          fi
          # Note: We always evaluate the LS_COLORS setting even when it's the
          # default.  If it isn't set, then `ls` will only colorize by default
          # based on file attributes and ignore extensions (even the compiled
          # in defaults of dircolors). #583814
          if [[ -n ${LS_COLORS:+set} ]] ; then
          use_color=true
          else
          # Delete it if it's empty as it's useless in that case.
          unset LS_COLORS
          fi
      else
          # Some systems (e.g. BSD & embedded) don't typically come with
          # dircolors so we need to hardcode some terminals in here.
          case ${TERM} in
          [aEkx]term*|rxvt*|gnome*|konsole*|screen|cons25|*color) use_color=true;;
          esac
      fi

      if [[ ${EUID} == 0 ]] ; then
        END_SYMBOL='#'
        USER_COLOUR_ON='{{ .HostOS.BashPrompt.RootUserColour.PCOn }}'
        USER_COLOUR_OFF='{{ .HostOS.BashPrompt.RootUserColour.PCOff }}'
        USERHOST_STYLE='{{ if .HostOS.BashPrompt.IncludeHostname }}\h{{ end }}'
        DIRECTORY_STYLE='\W'
      else
        END_SYMBOL='$'
        USER_COLOUR_ON='{{ .HostOS.BashPrompt.NonRootUserColour.PCOn }}'
        USER_COLOUR_OFF='{{ .HostOS.BashPrompt.NonRootUserColour.PCOff }}'
        USERHOST_STYLE='\u{{ if .HostOS.BashPrompt.IncludeHostname }}@\h{{ end }}'
        DIRECTORY_STYLE='\w'
      fi

      if ${use_color} ; then
          PS1+="{{ .HostOS.BashPrompt.ControllerColour.PCOn }}{{ .HostOS.BashPrompt.ControllerLabel }}{{ .HostOS.BashPrompt.ControllerColour.PCOff }}
          {{- .HostOS.BashPrompt.DividerColour.PCOn }}{{ .HostOS.BashPrompt.Divider }}{{ .HostOS.BashPrompt.DividerColour.PCOff }}
          {{- .HostOS.BashPrompt.ClusterColour.PCOn }}{{ .ClusterName }}{{ .HostOS.BashPrompt.ClusterColour.PCOff }}
          {{- if .HostOS.BashPrompt.IncludeUser }}${USER_COLOUR_ON} ${USERHOST_STYLE}${USER_COLOUR_OFF}{{ end }}
          {{- .HostOS.BashPrompt.DirectoryColour.PCOn }}
          {{- if .HostOS.BashPrompt.IncludePWD }} ${DIRECTORY_STYLE}{{ end }} ${END_SYMBOL} {{ .HostOS.BashPrompt.DirectoryColour.PCOff }}"

          alias ls='ls --color=auto'
          alias grep='grep --colour=auto'
          alias egrep='egrep --colour=auto'
          alias fgrep='fgrep --colour=auto'
      else
          PS1+="{{ .HostOS.BashPrompt.ControllerLabel }}
          {{- .HostOS.BashPrompt.Divider }}
          {{- .ClusterName }}
          {{- if .HostOS.BashPrompt.IncludeUser }} ${USERHOST_STYLE}{{ end }}
          {{- if .HostOS.BashPrompt.IncludePWD }} ${DIRECTORY_STYLE}{{ end }} ${END_SYMBOL} "
      fi

      for sh in /etc/bash/bashrc.d/* ; do
          [[ -r ${sh} ]] && source "${sh}"
      done

      # Try to keep environment pollution down, EPA loves us.
      unset use_color sh
  {{- end }}
{{ end }}
